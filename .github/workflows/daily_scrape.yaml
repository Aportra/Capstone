name: Daily Scraper

on:
  workflow_dispatch:
    inputs:
      branch:
        description: 'Branch to run the workflow on'
        required: true
        default: 'main'  # Trigger on a specific branch

env:
  ACTIONS_STEP_DEBUG: true

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout the repository
      - name: Checkout repository
        uses: actions/checkout@v3

      # # Step 2: Install Firefox and Dependencies
      # - name: Install Firefox and Dependencies
      #   run: |
      #     sudo apt-get update
      #     sudo apt-get install -y firefox
      #     sudo apt-get install -y \
      #       ca-certificates \
      #       fonts-liberation \
      #       libappindicator3-1 \
      #       libasound2 \
      #       libatk-bridge2.0-0 \
      #       libatk1.0-0 \
      #       libcups2 \
      #       libdbus-1-3 \
      #       libdrm2 \
      #       libgbm1 \
      #       libnspr4 \
      #       libnss3 \
      #       libx11-xcb1 \
      #       libxcomposite1 \
      #       libxdamage1 \
      #       libxrandr2 \
      #       xdg-utils \
      #       libu2f-udev

      # # Step 3: Install Geckodriver Manually
      # - name: Install Geckodriver Manually
      #   run: |
      #     GECKODRIVER_VERSION=$(curl -s https://api.github.com/repos/mozilla/geckodriver/releases/latest | jq -r '.tag_name')
      #     echo "Detected Geckodriver version: $GECKODRIVER_VERSION"
      #     wget -q "https://github.com/mozilla/geckodriver/releases/download/${GECKODRIVER_VERSION}/geckodriver-${GECKODRIVER_VERSION}-linux64.tar.gz"
      #     tar -xvzf geckodriver-${GECKODRIVER_VERSION}-linux64.tar.gz -C /usr/local/bin/
      #     rm geckodriver-${GECKODRIVER_VERSION}-linux64.tar.gz

      # # Step 4: Verify Firefox and Geckodriver Installation
      # - name: Verify Firefox and Geckodriver
      #   run: |
      #     firefox --version
      #     geckodriver --version
      #     which firefox
      #     which geckodriver
           # Step 2: Install Chrome and Dependencies
      - name: Install Older Chrome Version
        run: |
          sudo apt-get purge google-chrome-stable -y
          wget https://dl.google.com/linux/chrome/deb/pool/main/g/google-chrome-stable/google-chrome-stable_114.0.5735.90-1_amd64.deb
          sudo dpkg -i google-chrome-stable_114.0.5735.90-1_amd64.deb
      - name: Install ChromeDriver
        run: |
          CHROMEDRIVER_VERSION="116.0.5845.96"  # Closest available version
          echo "Using ChromeDriver version: $CHROMEDRIVER_VERSION"
          wget -q "https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip"
          unzip chromedriver_linux64.zip
          sudo mv chromedriver /usr/local/bin/
          rm chromedriver_linux64.zip

      # Step 3: Verify Chrome and ChromeDriver
      - name: Verify Chrome and ChromeDriver
        run: |
          which google-chrome
          which chromedriver
          google-chrome --version
          chromedriver --version
      # Step 4: Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      # Step 6: Install Python Dependencies
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 7: Run the Scraper
      - name: Run scraper
        run: python scrape_current_nba_games.py
        env:
          SERVER_EMAIL: ${{ secrets.SERVER_EMAIL }}
          EMAIL_USERNAME: ${{ secrets.EMAIL_USERNAME }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}

      # Step 8: Upload Screenshot
      - name: Upload Screenshot
        if: always()  # Always upload the artifact even if the scraper fails
        uses: actions/upload-artifact@v3
        with:
          name: screenshots
          path: screenshot.png

      # Step 9: Upload Logs
      - name: Upload Logs
        if: always()  # Always upload logs for debugging
        uses: actions/upload-artifact@v3
        with:
          name: logs
          path: scraper.log
