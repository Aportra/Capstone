{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.linear_model._logistic import LogisticRegression\n",
    "from datetime import datetime as dt\n",
    "from sklearn.model_selection import TimeSeriesSplit \n",
    "from sklearn.metrics import r2_score,make_scorer,mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import os\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_player_name(name):\n",
    "    \"\"\"Standardizes player names by removing special characters and handling known name variations.\"\"\"\n",
    "    name = name.lower().strip()  # Convert to lowercase & remove extra spaces\n",
    "    name = name.replace(\".\", \"\")  # Remove periods\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove special characters (apostrophes, dashes, etc.) \n",
    "    # Known name changes (add more as needed)\n",
    "    name_corrections = {\n",
    "        \"alexandre sarr\": \"alex sarr\",\n",
    "        \"jimmy butler\": \"jimmy butler iii\",\n",
    "        \"nicolas claxton\": \"nic claxton\",\n",
    "        \"kenyon martin jr\": \"kj martin\",\n",
    "        \"carlton carrington\": \"bub carrington\",\n",
    "        \"ron holland ii\": \"ronald holland ii\",\n",
    "        'cameron thomas':'cam thomas'\n",
    "    }\n",
    "\n",
    "    # Apply corrections if the name exists in the dictionary\n",
    "    return name_corrections.get(name, name)  # Default to original name if no correction found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#using shifted windows for rolling data to prevent data leakage\n",
    "player_query = f\"\"\" \n",
    "SELECT *\n",
    "from `capstone_data.player_modeling_data_partitioned`\n",
    "order by game_date asc\n",
    "\"\"\"\n",
    "\n",
    "team_query = f\"\"\"\n",
    "SELECT *\n",
    "from `capstone_data.team_modeling_data_partitioned`\n",
    "order by game_date asc\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68938/4285358681.py:2: DtypeWarning: Columns (126) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_data = pd.read_csv('full_data.csv')\n"
     ]
    }
   ],
   "source": [
    " \n",
    "try:\n",
    "    full_data = pd.read_csv('full_data.csv')\n",
    "\n",
    "except:\n",
    "    nba_player_data = pd.DataFrame(pandas_gbq.read_gbq(player_query,project_id='miscellaneous-projects-444203',progress_bar_type='tqdm'))\n",
    "    team_data = pd.DataFrame(pandas_gbq.read_gbq(team_query,project_id='miscellaneous-projects-444203',progress_bar_type='tqdm'))\n",
    "    team_data  = team_data.merge(team_data,on='game_id',suffixes=('',\"_opponent\"))\n",
    "    team_data = team_data[team_data[\"team_id\"] != team_data[\"team_id_opponent\"]]\n",
    "    full_data = nba_player_data.merge(team_data, on = ['game_id','team'], how = 'inner',suffixes=('','remove'))\n",
    "    full_data.drop([column for column in full_data.columns if 'remove' in column],axis = 1 , inplace=True) \n",
    "    full_data.drop([column for column in full_data.columns if '_1' in column],axis = 1 , inplace=True)\n",
    "    full_data.to_csv('full_data.csv',mode = 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered = full_data.sort_values('game_date')\n",
    "\n",
    "data_ordered.dropna(inplace=True,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Ideas \n",
    "\n",
    "* (ratio of 3pa and fga and 3pm and 3pa) TS% for players efg% \n",
    "* for players assist_to_turnover ratio assist ratio, \n",
    "* rebound_cahnce, defesnive reb %, \n",
    "* ast_ratio_season * pace, \n",
    "* home * pts season - data pts 3pm avg,\n",
    "* cold_streak pts_3gm_avg < pts_season boolean, \n",
    "* away difficulty away * opponent_defrtg_3gm_avg,\n",
    "* home_performance = data_ordered[data_ordered[\"home\"] == 1].groupby(\"team\")[\"pts_season\"].mean()\n",
    "* away_performance = data_ordered[data_ordered[\"away\"] == 1].groupby(\"team\")[\"pts_season\"].mean() these would be to see how the team performance changes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68938/3943372088.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data_ordered = data_ordered.groupby(['player','season']).apply(lambda x: x.iloc[3:]).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "data_ordered = data_ordered.groupby(['player','season']).apply(lambda x: x.iloc[3:]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered.sort_values(by='game_date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['game_date'] = pd.to_datetime(data_ordered['game_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['days_ago'] = (data_ordered['game_date'].max() - data_ordered['game_date']).dt.days\n",
    "data_ordered['time_decay_weight'] = 1 / (1 + np.log(1 + data_ordered['days_ago']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_ordered = data_ordered.drop('Unnamed: 0', axis =1)\n",
    "except KeyError:\n",
    "    print('Irregular column not made')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs with the column mean, but only for numeric columns\n",
    "data_ordered.fillna(data_ordered.select_dtypes(include=['number']).mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = data_ordered.select_dtypes(include=['number']).columns.tolist()\n",
    "numeric_columns = [column for column in numeric_columns if column not in ['pts','reb','ast','blk','stl','3pm','game_id','game_date','days_ago','time_decay_weight','team_id', \"gp_rank\", \"w_rank\", \"l_rank\", \"w_pct_rank\", \"min_rank\", \"fgm_rank\",\n",
    "    \"fga_rank\", \"fg_pct_rank\", \"fg3m_rank\", \"fg3a_rank\", \"fg3_pct_rank\",\n",
    "    \"ftm_rank\", \"fta_rank\", \"ft_pct_rank\", \"oreb_rank\", \"dreb_rank\",\n",
    "    \"reb_rank\", \"ast_rank\", \"tov_rank\", \"stl_rank\", \"blk_rank\",\n",
    "    \"blka_rank\", \"pf_rank\", \"pfd_rank\", \"pts_rank\", \"plus_minus_rank\",]]\n",
    "\n",
    "numeric_columns = [feature for feature in numeric_columns if any(keyword in feature for keyword in [\"3gm_avg\", \"season\", \"momentum\"])]\n",
    "features = {feature:[] for feature in ['pts','reb','ast','3pm']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m train_data[numeric_columns]\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m train_data[category]\n\u001b[0;32m----> 5\u001b[0m mi_scores \u001b[38;5;241m=\u001b[39m \u001b[43mmutual_info_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m mi_scores \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(mi_scores, index\u001b[38;5;241m=\u001b[39mnumeric_columns)\n\u001b[1;32m      7\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m mi_scores[mi_scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.10\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()  \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:441\u001b[0m, in \u001b[0;36mmutual_info_regression\u001b[0;34m(X, y, discrete_features, n_neighbors, copy, random_state, n_jobs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    326\u001b[0m     {\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    346\u001b[0m ):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate mutual information for a continuous target variable.\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    Mutual information (MI) [1]_ between two random variables is a non-negative\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    array([0.1..., 2.6...  , 0.0...])\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_estimate_mi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiscrete_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscrete_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiscrete_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:317\u001b[0m, in \u001b[0;36m_estimate_mi\u001b[0;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state, n_jobs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     y \u001b[38;5;241m=\u001b[39m scale(y, with_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    311\u001b[0m     y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;241m1e-10\u001b[39m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m1\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(y)))\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;241m*\u001b[39m rng\u001b[38;5;241m.\u001b[39mstandard_normal(size\u001b[38;5;241m=\u001b[39mn_samples)\n\u001b[1;32m    315\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m mi \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_compute_mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_feature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_iterate_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(mi)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:169\u001b[0m, in \u001b[0;36m_compute_mi\u001b[0;34m(x, y, x_discrete, y_discrete, n_neighbors)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compute_mi_cd(x, y, n_neighbors)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compute_mi_cc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:69\u001b[0m, in \u001b[0;36m_compute_mi_cc\u001b[0;34m(x, y, n_neighbors)\u001b[0m\n\u001b[1;32m     66\u001b[0m nx \u001b[38;5;241m=\u001b[39m kd\u001b[38;5;241m.\u001b[39mquery_radius(x, radius, count_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     67\u001b[0m nx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(nx) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m---> 69\u001b[0m kd \u001b[38;5;241m=\u001b[39m \u001b[43mKDTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchebyshev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m ny \u001b[38;5;241m=\u001b[39m kd\u001b[38;5;241m.\u001b[39mquery_radius(y, radius, count_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m ny \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(ny) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for category in features.keys():\n",
    "    x = train_data[numeric_columns]\n",
    "    y = train_data[category]\n",
    "\n",
    "    mi_scores = mutual_info_regression(x, y)\n",
    "    mi_scores = pd.Series(mi_scores, index=numeric_columns)\n",
    "    selected_features = mi_scores[mi_scores > 0.10].index.tolist()  \n",
    "\n",
    "    features[category] = selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models = {category:{} for category in ['pts','reb','ast','3pm']} \n",
    "saved_results = {category:{} for category in ['pts','reb','ast','3pm']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP\n",
    "Applying shap to help reduce collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for category in features.keys():\n",
    "\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    print(len(features_list))\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    linear_model = LinearRegression()\n",
    "\n",
    "    linear_model.fit(x_train,y_train)\n",
    "\n",
    "    y_pred = linear_model.predict(x_test)\n",
    "    print(category)\n",
    "    print(r2_score(y_true=y_test,y_pred=y_pred))\n",
    "\n",
    "    saved_results[category]['linear_model']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n",
    "    saved_models[category]['linear_model'] = linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in features.keys():\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    ridge_model = Ridge(alpha=1)\n",
    "\n",
    "    ridge_model.fit(x_train,y_train)\n",
    "\n",
    "    output = pd.DataFrame({'prediction':ridge_model.predict(x_test), 'actual':y_test})\n",
    "    print(category)\n",
    "    print(r2_score(y_true=output['actual'],y_pred=output['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for category in features.keys():\n",
    "    features_list = features[category]\n",
    "\n",
    "    x_train, y_train = train_data[features_list], train_data[category]\n",
    "    x_test, y_test = test_data[features_list], test_data[category]\n",
    "\n",
    "    linear_model = Ridge(alpha=1.0)  # Use Ridge instead of LinearRegression\n",
    "    linear_model.fit(x_train, y_train)\n",
    "\n",
    "    # Cross-validation score instead of just test R²\n",
    "    cv_r2 = cross_val_score(linear_model, x_train, y_train, cv=5, scoring='r2').mean()\n",
    "\n",
    "    y_pred = linear_model.predict(x_test)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{category}: Cross-Val R² = {cv_r2:.4f}, Test R² = {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(data_ordered[numeric_columns])\n",
    "\n",
    "scaled_data_df = pd.DataFrame(scaled_data,columns=numeric_columns)\n",
    "\n",
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "scaled_train_data = scaled_data_df.iloc[:split_index]\n",
    "scaled_test_data = scaled_data_df[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth':[3,6,9],'learning_rate':[.01,.05,.1,.3],'booster':['gbtree','dart'],'subsample':[.5,.7,.9],'colsample_bytree':[.5,.7,.9],'n_estimators':[100,300,500]}\n",
    "param_linear = {'booster':['gblinear'],'lambda':[0,.1,1,10],'alpha':[0,.1,1,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor = xgboost.XGBRegressor()\n",
    "mse_score = make_scorer(mean_squared_error,greater_is_better=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "scoring = {'MSE':mse_score,'r2':r2_scorer}\n",
    "grid_search = GridSearchCV(estimator=xgb_regressor,param_grid=param_grid,scoring = scoring,cv=tscv,n_jobs=1,verbose=0,refit='r2')\n",
    "grid_linear_search = GridSearchCV(estimator=xgb_regressor,param_grid=param_linear,scoring = scoring,cv=tscv,n_jobs=3,verbose=0,refit='r2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_features =  features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in features.keys():\n",
    "    x_train,y_train = scaled_train_data[xg_features[category]],train_data[category]\n",
    "    x_test, y_test = scaled_test_data[xg_features[category]],test_data[category]\n",
    "\n",
    "    fit_params = {'eval_set':[(x_test,y_test)],'early_stopping_rounds':20,'verbose':False}\n",
    "\n",
    "    grid_linear_search.estimator.set_params(eval_metric='rmse')\n",
    "\n",
    "\n",
    "    grid_linear_search.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "    print(category)\n",
    "    print(grid_linear_search.best_params_)\n",
    "    print(grid_linear_search.best_score_)\n",
    "\n",
    "    y_pred = grid_linear_search.best_estimator_.predict(x_test)\n",
    "\n",
    "    saved_models[category]['XGboost'] = grid_linear_search.best_estimator_\n",
    "    saved_results[category]['XGboost']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "lgb_model = LGBMRegressor(n_estimators=1000, random_state=42,verbosity=-1)\n",
    "\n",
    "# Define the expanded parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [15, 31, 50, 75],\n",
    "    'learning_rate': [0.005, 0.01, 0.05],\n",
    "    'max_depth': [-1, 5, 10, 15],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Time series split (if your data is chronological)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Randomized search setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=40,  # control number of total combinations to test\n",
    "    cv=tscv,\n",
    "    scoring='r2',\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit to your training data\n",
    "# Best model + params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]\n",
    "for category in features.keys():\n",
    "    x_train,y_train = train_data[features[category]],train_data[category]\n",
    "    x_test,y_test = test_data[features[category]],test_data[category]\n",
    "\n",
    "    random_search.fit(x_train,y_train)\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    print(category)\n",
    "    print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "\n",
    "    saved_models[category]['lightgbm'] = best_model\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'R2: {r2}')\n",
    "\n",
    "    saved_results[category]['lightgbm']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(saved_models,'models.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_performance.txt', 'w') as file:\n",
    "    for category, models in saved_results.items():\n",
    "        file.write(f\"Category: {category}\\n\")\n",
    "        for model, metrics in models.items():\n",
    "            file.write(f\"  Model: {model}\\n\")\n",
    "            for metric, value in metrics.items():\n",
    "                file.write(f\"    {metric}: {value}\\n\")\n",
    "        file.write(\"\\n\")  # Newline between categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Modeling into Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Ensemble modeling\n",
    "\n",
    "saved_models = joblib.load('models.pkl')\n",
    "\n",
    "linear_models = {cat: saved_models[cat]['linear_model'] for cat in saved_models if 'linear_model' in saved_models[cat]}\n",
    "lightgbm_models = {cat: saved_models[cat]['lightgbm'] for cat in saved_models if 'lightgbm' in saved_models[cat]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts Meta-model R²: 0.6164, MSE: 30.4233\n",
      "reb Meta-model R²: 0.5308, MSE: 5.6493\n",
      "ast Meta-model R²: 0.5695, MSE: 3.0275\n",
      "3pm Meta-model R²: 0.4488, MSE: 1.2632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "meta_models = {}\n",
    "meta_results = {}\n",
    "\n",
    "for category in ['pts', 'reb', 'ast', '3pm']:\n",
    "    # Load models\n",
    "    lm = saved_models[category]['linear_model']\n",
    "    lgb = saved_models[category]['lightgbm']\n",
    "\n",
    "    # Prepare test data\n",
    "    lm_features_list = [f.strip() for f in lm.feature_names_in_]\n",
    "    lgb_features_list = [f.strip() for f in lgb.feature_names_in_]\n",
    "\n",
    "    lm_x_test = test_data[lm_features_list]\n",
    "\n",
    "    lgb_x_test = test_data[lgb_features_list]\n",
    "\n",
    "    y_test = test_data[category]\n",
    "\n",
    "    # Get predictions\n",
    "    preds_lm = lm.predict(lm_x_test)\n",
    "    preds_lgb = lgb.predict(lgb_x_test)\n",
    "\n",
    "    # Stack predictions into meta-model features\n",
    "    meta_X = np.vstack([preds_lm, preds_lgb]).T\n",
    "    meta_y = y_test.values\n",
    "\n",
    "    # Train meta-model\n",
    "    meta_model = Ridge()\n",
    "    meta_model.fit(meta_X, meta_y)\n",
    "\n",
    "    # Evaluate meta-model\n",
    "    meta_preds = meta_model.predict(meta_X)\n",
    "    r2 = r2_score(meta_y, meta_preds)\n",
    "    mse = mean_squared_error(meta_y, meta_preds)\n",
    "\n",
    "    print(f\"{category} Meta-model R²: {r2:.4f}, MSE: {mse:.4f}\")\n",
    "    \n",
    "    meta_models[category] = meta_model\n",
    "    meta_results[category] = {'r2': r2, 'mse': mse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta_model.pkl']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(meta_models,'meta_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTS Meta-Model Weights:\n",
      "  Linear Model Weight:  -0.1503\n",
      "  LightGBM Weight:      1.1462\n",
      "REB Meta-Model Weights:\n",
      "  Linear Model Weight:  0.1157\n",
      "  LightGBM Weight:      0.8922\n",
      "AST Meta-Model Weights:\n",
      "  Linear Model Weight:  0.1427\n",
      "  LightGBM Weight:      0.8560\n",
      "3PM Meta-Model Weights:\n",
      "  Linear Model Weight:  -0.2429\n",
      "  LightGBM Weight:      1.2381\n"
     ]
    }
   ],
   "source": [
    "coef = {}\n",
    "for category, model in meta_models.items():\n",
    "    coef_linear, coef_lgbm = model.coef_\n",
    "    print(f\"{category.upper()} Meta-Model Weights:\")\n",
    "    print(f\"  Linear Model Weight:  {coef_linear:.4f}\")\n",
    "    print(f\"  LightGBM Weight:      {coef_lgbm:.4f}\")\n",
    "\n",
    "    coef[f'{category}_lm'] = coef_linear\n",
    "    coef[f'{category}_lgb'] = coef_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Model into Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_profit(pred, actual, odds):\n",
    "    if pred == actual:\n",
    "        return 100 if odds < 0 else odds\n",
    "    else:\n",
    "        return -abs(odds) if odds < 0 else -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "cats = ['points','assists','rebounds','threes_made']\n",
    "categories = ['pts','ast','reb','3pm']\n",
    "odds_data = {}\n",
    "for cat,category in zip(cats,categories):\n",
    "    predictions_query = f\"\"\"\n",
    "    select \n",
    "        po.*,\n",
    "        pp.Over,\n",
    "        pp.Under,\n",
    "        pp.{category}_linear_model,\n",
    "        pp.{category}_lightgbm\n",
    "    from `capstone_data.{cat}_predictions` pp\n",
    "    inner join `capstone_data.{category}_outcome` po\n",
    "        on pp.Player = po.player and date(pp.Date_Updated) = po.game_date\n",
    "    where pp.{category}_linear_model is not null\n",
    "    \"\"\" \n",
    "    data = pandas_gbq.read_gbq(predictions_query,project_id='miscellaneous-projects-444203')\n",
    "\n",
    "    \n",
    "    odds_data[category] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-03-28 00:00:00')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ordered['game_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = {category:[] for category in categories}\n",
    "\n",
    "data_ordered['player'] = data_ordered['player'].apply(clean_player_name)\n",
    "\n",
    "for category in categories:\n",
    "\n",
    "    data = odds_data[category]\n",
    "    \n",
    "    data['game_date'] = pd.to_datetime(data['game_date'])\n",
    "\n",
    "    data['player']= data['player'].apply(clean_player_name)\n",
    "\n",
    "    data = data.merge(data_ordered, on=['player','game_date'],how='inner')\n",
    "\n",
    "    data = data.drop_duplicates(subset=['player', 'game_date'], keep='first')\n",
    "\n",
    "    full_data[category] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat,category in zip(cats,categories):\n",
    "    full_data[category][f'{category}_ensemble'] = (pd.to_numeric(full_data[category][f'{category}_linear_model']) * \n",
    "    coef[f'{category}_lm'] + pd.to_numeric(full_data[category][f'{category}_lightgbm'] * coef[f'{category}_lgb']))\n",
    "\n",
    "    full_data[category][f'{category}_delta'] = full_data[category][f'{cat}'] - full_data[category][f'{category}_ensemble']\n",
    "\n",
    "    full_data[category].sort_values(by='game_date',inplace=True)\n",
    "\n",
    "    full_data[category].drop('time_decay_weight',axis = 1, inplace = True)\n",
    "    full_data[category].drop('days_ago',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player\n",
      "points\n",
      "pts_x\n",
      "game_date\n",
      "result\n",
      "recommendation_pts_linear_model\n",
      "recommendation_pts_lightgbm\n",
      "Over\n",
      "Under\n",
      "pts_linear_model\n",
      "pts_lightgbm\n",
      "game_id\n",
      "team_id\n",
      "team\n",
      "team_city\n",
      "player_id\n",
      "min\n",
      "fgm\n",
      "fga\n",
      "fg_pct\n",
      "3pm\n",
      "fg3a\n",
      "fg3_pct\n",
      "ftm\n",
      "fta\n",
      "ft_pct\n",
      "oreb\n",
      "dreb\n",
      "reb\n",
      "ast\n",
      "stl\n",
      "blk\n",
      "to\n",
      "pf\n",
      "pts_y\n",
      "plus_minus\n",
      "season\n",
      "min_3gm_avg\n",
      "min_season\n",
      "min_momentum\n",
      "fgm_3gm_avg\n",
      "fgm_season\n",
      "fgm_momentum\n",
      "fga_3gm_avg\n",
      "fga_season\n",
      "fga_momentum\n",
      "fg_pct_3gm_avg\n",
      "fg_pct_season\n",
      "fg_pct_momentum\n",
      "fg3m_3gm_avg\n",
      "fg3m_season\n",
      "fg3m_momentum\n",
      "fg3a_3gm_avg\n",
      "fg3a_season\n",
      "fg3a_momentum\n",
      "fg3_pct_3gm_avg\n",
      "fg3_pct_season\n",
      "fg3_pct_momentum\n",
      "ftm_3gm_avg\n",
      "ftm_season\n",
      "ftm_momentum\n",
      "fta_3gm_avg\n",
      "fta_season\n",
      "fta_momentum\n",
      "ft_pct_3gm_avg\n",
      "ft_pct_season\n",
      "ft_pct_momentum\n",
      "oreb_3gm_avg\n",
      "oreb_season\n",
      "oreb_momentum\n",
      "dreb_3gm_avg\n",
      "dreb_season\n",
      "dreb_momentum\n",
      "reb_3gm_avg\n",
      "reb_season\n",
      "reb_momentum\n",
      "ast_3gm_avg\n",
      "ast_season\n",
      "ast_momentum\n",
      "stl_3gm_avg\n",
      "stl_season\n",
      "stl_momentum\n",
      "blk_3gm_avg\n",
      "blk_season\n",
      "blk_momentum\n",
      "to_3gm_avg\n",
      "to_season\n",
      "to_momentum\n",
      "pf_3gm_avg\n",
      "pf_season\n",
      "pf_momentum\n",
      "pts_3gm_avg\n",
      "pts_season\n",
      "pts_momentum\n",
      "plus_minus_3gm_avg\n",
      "plus_minus_season\n",
      "plus_minus_momentum\n",
      "season_start_year\n",
      "season_id\n",
      "team_name\n",
      "matchup\n",
      "wl\n",
      "fg3m\n",
      "tov\n",
      "tov_3gm_avg\n",
      "tov_season\n",
      "tov_momentum\n",
      "season_id_opponent\n",
      "team_id_opponent\n",
      "team_opponent\n",
      "team_name_opponent\n",
      "game_date_opponent\n",
      "matchup_opponent\n",
      "wl_opponent\n",
      "min_opponent\n",
      "fgm_opponent\n",
      "fga_opponent\n",
      "fg_pct_opponent\n",
      "fg3m_opponent\n",
      "fg3a_opponent\n",
      "fg3_pct_opponent\n",
      "ftm_opponent\n",
      "fta_opponent\n",
      "ft_pct_opponent\n",
      "oreb_opponent\n",
      "dreb_opponent\n",
      "reb_opponent\n",
      "ast_opponent\n",
      "stl_opponent\n",
      "blk_opponent\n",
      "tov_opponent\n",
      "pf_opponent\n",
      "pts_opponent\n",
      "plus_minus_opponent\n",
      "season_start_year_opponent\n",
      "min_3gm_avg_opponent\n",
      "min_season_opponent\n",
      "min_momentum_opponent\n",
      "fgm_3gm_avg_opponent\n",
      "fgm_season_opponent\n",
      "fgm_momentum_opponent\n",
      "fga_3gm_avg_opponent\n",
      "fga_season_opponent\n",
      "fga_momentum_opponent\n",
      "fg_pct_3gm_avg_opponent\n",
      "fg_pct_season_opponent\n",
      "fg_pct_momentum_opponent\n",
      "fg3m_3gm_avg_opponent\n",
      "fg3m_season_opponent\n",
      "fg3m_momentum_opponent\n",
      "fg3a_3gm_avg_opponent\n",
      "fg3a_season_opponent\n",
      "fg3a_momentum_opponent\n",
      "fg3_pct_3gm_avg_opponent\n",
      "fg3_pct_season_opponent\n",
      "fg3_pct_momentum_opponent\n",
      "ftm_3gm_avg_opponent\n",
      "ftm_season_opponent\n",
      "ftm_momentum_opponent\n",
      "fta_3gm_avg_opponent\n",
      "fta_season_opponent\n",
      "fta_momentum_opponent\n",
      "ft_pct_3gm_avg_opponent\n",
      "ft_pct_season_opponent\n",
      "ft_pct_momentum_opponent\n",
      "oreb_3gm_avg_opponent\n",
      "oreb_season_opponent\n",
      "oreb_momentum_opponent\n",
      "dreb_3gm_avg_opponent\n",
      "dreb_season_opponent\n",
      "dreb_momentum_opponent\n",
      "reb_3gm_avg_opponent\n",
      "reb_season_opponent\n",
      "reb_momentum_opponent\n",
      "ast_3gm_avg_opponent\n",
      "ast_season_opponent\n",
      "ast_momentum_opponent\n",
      "stl_3gm_avg_opponent\n",
      "stl_season_opponent\n",
      "stl_momentum_opponent\n",
      "blk_3gm_avg_opponent\n",
      "blk_season_opponent\n",
      "blk_momentum_opponent\n",
      "tov_3gm_avg_opponent\n",
      "tov_season_opponent\n",
      "tov_momentum_opponent\n",
      "pf_3gm_avg_opponent\n",
      "pf_season_opponent\n",
      "pf_momentum_opponent\n",
      "pts_3gm_avg_opponent\n",
      "pts_season_opponent\n",
      "pts_momentum_opponent\n",
      "plus_minus_3gm_avg_opponent\n",
      "plus_minus_season_opponent\n",
      "plus_minus_momentum_opponent\n",
      "days_ago\n",
      "pts_ensemble\n",
      "pts_delta\n"
     ]
    }
   ],
   "source": [
    "for column in full_data['pts']:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing category: PTS ===\n",
      "Top averaged MI features for PTS:\n",
      "['plus_minus_momentum', 'pts_lightgbm', 'pts_momentum', 'fga_momentum', 'ftm_season', 'dreb_momentum', 'ft_pct_momentum', 'fg_pct_momentum', 'fta_opponent', 'fg3_pct_momentum', 'fg3a_season', 'pts_3gm_avg', 'fg3a_opponent', 'Over', 'pf_momentum']\n",
      "\n",
      "=== Processing category: AST ===\n",
      "Top averaged MI features for AST:\n",
      "['ftm_season', 'oreb_season', 'fg3a_season', 'ast_lightgbm', 'reb_momentum', 'dreb_momentum', 'oreb_momentum', 'fta_season_opponent', 'fg_pct_3gm_avg', 'Over', 'pts_season', 'min_3gm_avg', 'ast_3gm_avg', 'min_season', 'plus_minus_season']\n",
      "\n",
      "=== Processing category: REB ===\n",
      "Top averaged MI features for REB:\n",
      "['ftm_season', 'fta_momentum', 'blk_momentum', 'fg3m_season', 'reb_linear_model', 'fgm_opponent', 'fta_3gm_avg', 'fg_pct_season', 'ft_pct_momentum', 'fgm_season_opponent', 'oreb_3gm_avg', 'dreb_season', 'fg3a_season', 'min_momentum_opponent', 'fg3a_opponent']\n",
      "\n",
      "=== Processing category: 3PM ===\n",
      "Top averaged MI features for 3PM:\n",
      "['dreb_momentum', 'ast_momentum', 'ftm_season', 'fg3m', 'blk_season', 'fg_pct_season', 'ft_pct_3gm_avg', 'ftm_opponent', 'to_season', 'fg3_pct_momentum', 'to_3gm_avg', 'pf_season', 'ast_season', 'season_id_opponent', 'blk_momentum']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "classification_features = {}\n",
    "mi_feature_sets = {}\n",
    "categories = ['pts', 'ast', 'reb', '3pm']\n",
    "\n",
    "box_score_cols = [\n",
    "    'pts', 'pts_y', 'pts_x', 'ast_x', 'ast_y', 'reb_x', 'reb_y', 'reb', 'ast',\n",
    "    'stl', 'blk', 'to', 'pf', 'min', 'fgm', 'fga', 'fg_pct',\n",
    "    '3pm_y', 'fg3a', 'fg3_pct', 'ftm', 'fta', 'ft_pct',\n",
    "    'oreb', 'dreb', 'plus_minus', '3pm', '3pm_x'\n",
    "]\n",
    "exclude_cols = box_score_cols + ['result', 'player', 'game_date', 'team', 'team_city', 'matchup', 'matchup_opponent']\n",
    "\n",
    "NUM_RUNS = 10\n",
    "SEEDS = list(range(42, 42 + NUM_RUNS))\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\n=== Processing category: {category.upper()} ===\")\n",
    "    data = full_data[category].copy()\n",
    "\n",
    "    if isinstance(data['result'].iloc[0], str):\n",
    "        data['result'] = data['result'].map({'Over': 1, 'Under': 0})\n",
    "\n",
    "    data = data.dropna(subset=['result'])\n",
    "\n",
    "    # Define features\n",
    "    candidate_features = [col for col in data.columns if col not in exclude_cols]\n",
    "    numeric_features = [col for col in candidate_features if pd.api.types.is_numeric_dtype(data[col])]\n",
    "\n",
    "    X = data[numeric_features].fillna(0)\n",
    "    y = data['result']\n",
    "\n",
    "    # ➤ Split first (no shuffle to preserve game ordering if time-based)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    # ➤ Standard scale only on training set\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # ➤ MI on training data only\n",
    "    mi_matrix = []\n",
    "    for seed in SEEDS:\n",
    "        mi_scores = mutual_info_classif(X_train_scaled, y_train, random_state=seed)\n",
    "        mi_matrix.append(mi_scores)\n",
    "\n",
    "    mi_avg_scores = np.mean(mi_matrix, axis=0)\n",
    "    mi_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Avg_MI_Score': mi_avg_scores\n",
    "    }).sort_values(by='Avg_MI_Score', ascending=False)\n",
    "\n",
    "    # ➤ Select top 15 features\n",
    "    top_features = mi_df['Feature'].head(15).tolist()\n",
    "    mi_feature_sets[category] = top_features\n",
    "    classification_features[category] = top_features\n",
    "\n",
    "    print(f\"Top averaged MI features for {category.upper()}:\")\n",
    "    print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category in classification_features.keys():\n",
    "    for i in classification_features[category]:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: RandomForest ===\n",
      "\n",
      "--- Category: PTS ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     299     |  0.5552 |   1196.00  |    3.74\n",
      "    0.60     |     0.40     |     286     |  0.5559 |   1186.00  |    3.87\n",
      "    0.65     |     0.35     |     275     |  0.5673 |   1791.00  |    6.09\n",
      "    0.70     |     0.30     |     263     |  0.5665 |   1661.00  |    5.90\n",
      "    0.75     |     0.25     |     242     |  0.5661 |   1501.00  |    5.79\n",
      "\n",
      "--- Category: AST ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     201     |  0.5871 |   1495.00  |    6.58\n",
      "    0.60     |     0.40     |     190     |  0.5789 |   985.00  |    4.56\n",
      "    0.65     |     0.35     |     186     |  0.5806 |   1030.00  |    4.87\n",
      "    0.70     |     0.30     |     174     |  0.5747 |   685.00  |    3.46\n",
      "    0.75     |     0.25     |     168     |  0.5833 |   940.00  |    4.92\n",
      "\n",
      "--- Category: REB ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     286     |  0.5140 |   -1535.00  |   -4.79\n",
      "    0.60     |     0.40     |     273     |  0.5201 |   -1025.00  |   -3.37\n",
      "    0.65     |     0.35     |     264     |  0.5189 |   -1017.00  |   -3.46\n",
      "    0.70     |     0.30     |     253     |  0.5178 |   -974.00  |   -3.46\n",
      "    0.75     |     0.25     |     237     |  0.5021 |   -1735.00  |   -6.55\n",
      "\n",
      "--- Category: 3PM ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     200     |  0.5700 |   1954.00  |    8.58\n",
      "    0.60     |     0.40     |     193     |  0.5751 |   2084.00  |    9.47\n",
      "    0.65     |     0.35     |     186     |  0.5860 |   2399.00  |   11.33\n",
      "    0.70     |     0.30     |     178     |  0.5899 |   2380.00  |   11.71\n",
      "    0.75     |     0.25     |     171     |  0.5906 |   2236.00  |   11.42\n",
      "\n",
      "=== Model: LogisticRegression ===\n",
      "\n",
      "--- Category: PTS ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     173     |  0.5723 |   1206.00  |    6.49\n",
      "    0.60     |     0.40     |      92     |  0.5652 |   471.00  |    4.74\n",
      "    0.65     |     0.35     |      29     |  0.6207 |   525.00  |   17.07\n",
      "    0.70     |     0.30     |      10     |  0.7000 |   345.00  |   32.70\n",
      "    0.75     |     0.25     |       3     |  1.0000 |   300.00  |  100.00\n",
      "\n",
      "--- Category: AST ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     118     |  0.5763 |   189.00  |    1.39\n",
      "    0.60     |     0.40     |      49     |  0.6327 |   663.00  |   11.80\n",
      "    0.65     |     0.35     |      15     |  0.6000 |    22.00  |    1.21\n",
      "    0.70     |     0.30     |       4     |  0.7500 |    62.00  |   11.52\n",
      "    0.75     |     0.25     |       1     |  0.0000 |   -238.00  |  -100.00\n",
      "\n",
      "--- Category: REB ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     138     |  0.5362 |   145.00  |    0.96\n",
      "    0.60     |     0.40     |      57     |  0.5439 |   183.00  |    2.95\n",
      "    0.65     |     0.35     |      23     |  0.6957 |   884.00  |   36.99\n",
      "    0.70     |     0.30     |       3     |  0.3333 |   -125.00  |  -38.46\n",
      "\n",
      "--- Category: 3PM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     164     |  0.6220 |   3404.00  |   18.44\n",
      "    0.60     |     0.40     |     119     |  0.6218 |   2421.00  |   18.03\n",
      "    0.65     |     0.35     |      78     |  0.6667 |   2664.00  |   31.05\n",
      "    0.70     |     0.30     |      38     |  0.8947 |   3334.00  |   85.71\n",
      "    0.75     |     0.25     |      22     |  0.8636 |   1809.00  |   79.69\n",
      "\n",
      "=== Model: LightGBM ===\n",
      "\n",
      "--- Category: PTS ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     306     |  0.5425 |   436.00  |    1.33\n",
      "    0.60     |     0.40     |     298     |  0.5403 |   300.00  |    0.94\n",
      "    0.65     |     0.35     |     290     |  0.5379 |   150.00  |    0.48\n",
      "    0.70     |     0.30     |     282     |  0.5426 |   445.00  |    1.47\n",
      "    0.75     |     0.25     |     274     |  0.5474 |   735.00  |    2.51\n",
      "\n",
      "--- Category: AST ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     205     |  0.5610 |   917.00  |    3.98\n",
      "    0.60     |     0.40     |     199     |  0.5578 |   765.00  |    3.42\n",
      "    0.65     |     0.35     |     194     |  0.5515 |   481.00  |    2.20\n",
      "    0.70     |     0.30     |     188     |  0.5585 |   697.00  |    3.30\n",
      "    0.75     |     0.25     |     182     |  0.5549 |   497.00  |    2.42\n",
      "\n",
      "--- Category: REB ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     285     |  0.5263 |   -360.00  |   -1.14\n",
      "    0.60     |     0.40     |     274     |  0.5219 |   -550.00  |   -1.81\n",
      "    0.65     |     0.35     |     266     |  0.5338 |   140.00  |    0.48\n",
      "    0.70     |     0.30     |     259     |  0.5328 |    70.00  |    0.24\n",
      "    0.75     |     0.25     |     245     |  0.5347 |   205.00  |    0.76\n",
      "\n",
      "--- Category: 3PM ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     201     |  0.5473 |   852.00  |    3.70\n",
      "    0.60     |     0.40     |     196     |  0.5408 |   492.00  |    2.19\n",
      "    0.65     |     0.35     |     194     |  0.5412 |   507.00  |    2.27\n",
      "    0.70     |     0.30     |     191     |  0.5340 |   207.00  |    0.94\n",
      "    0.75     |     0.25     |     186     |  0.5430 |   483.00  |    2.25\n",
      "\n",
      "=== Model: Stacked ===\n",
      "\n",
      "--- Category: PTS ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     287     |  0.5192 |   -1154.00  |   -3.73\n",
      "    0.60     |     0.40     |     265     |  0.5321 |   -334.00  |   -1.17\n",
      "    0.65     |     0.35     |     240     |  0.5375 |   -34.00  |   -0.13\n",
      "    0.70     |     0.30     |     213     |  0.5399 |    31.00  |    0.13\n",
      "    0.75     |     0.25     |     185     |  0.5351 |   -214.00  |   -1.07\n",
      "\n",
      "--- Category: AST ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     209     |  0.5407 |    66.00  |    0.28\n",
      "    0.60     |     0.40     |     205     |  0.5366 |   -74.00  |   -0.32\n",
      "    0.65     |     0.35     |     203     |  0.5419 |   161.00  |    0.70\n",
      "    0.70     |     0.30     |     199     |  0.5427 |   137.00  |    0.61\n",
      "    0.75     |     0.25     |     192     |  0.5417 |    57.00  |    0.26\n",
      "\n",
      "--- Category: REB ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     293     |  0.5188 |   -1043.00  |   -3.20\n",
      "    0.60     |     0.40     |     286     |  0.5245 |   -663.00  |   -2.08\n",
      "    0.65     |     0.35     |     277     |  0.5235 |   -658.00  |   -2.14\n",
      "    0.70     |     0.30     |     269     |  0.5242 |   -583.00  |   -1.95\n",
      "    0.75     |     0.25     |     258     |  0.5349 |   104.00  |    0.36\n",
      "\n",
      "--- Category: 3PM ---\n",
      "Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\n",
      "---------------------------------------------------------------------------\n",
      "    0.55     |     0.45     |     196     |  0.5561 |   1010.00  |    4.49\n",
      "    0.60     |     0.40     |     189     |  0.5661 |   1315.00  |    6.05\n",
      "    0.65     |     0.35     |     184     |  0.5707 |   1500.00  |    7.09\n",
      "    0.70     |     0.30     |     177     |  0.5706 |   1420.00  |    6.95\n",
      "    0.75     |     0.25     |     166     |  0.5723 |   1235.00  |    6.43\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "THRESH_OVER = 0.65\n",
    "THRESH_UNDER = 0.35\n",
    "label_map = {'Over': 1, 'Under': 0}\n",
    "\n",
    "# Define model options\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=150, max_depth=8, random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42),\n",
    "    'Stacked': StackingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=100, random_state=42)),\n",
    "        ],\n",
    "        final_estimator=LogisticRegression()\n",
    "    )\n",
    "}\n",
    "\n",
    "# Results storage\n",
    "results = {}\n",
    "\n",
    "# Helper for American odds payout\n",
    "def compute_profit(pred, actual, odds):\n",
    "    if pred == actual:\n",
    "        return 100 if odds < 0 else odds\n",
    "    else:\n",
    "        return -abs(odds) if odds < 0 else -100\n",
    "\n",
    "# Loop through models and categories\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n=== Model: {model_name} ===\")\n",
    "    results[model_name] = {}\n",
    "    \n",
    "    for category in ['pts', 'ast', 'reb', '3pm']:\n",
    "        print(f\"\\n--- Category: {category.upper()} ---\")\n",
    "        df = full_data[category].copy()\n",
    "        df = df.dropna(subset=['result'])\n",
    "        df['result'] = df['result'].map(label_map) if df['result'].dtype == object else df['result']\n",
    "\n",
    "        x = df[classification_features[category]].fillna(0)\n",
    "        y = df['result']\n",
    "        over_odds = df['Over'].values\n",
    "        under_odds = df['Under'].values\n",
    "\n",
    "        # Train-test split\n",
    "        x_train, x_test, y_train, y_test, over_train, over_test, under_train, under_test = train_test_split(\n",
    "            x, y, over_odds, under_odds, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Fit + calibrate\n",
    "        base_model = model\n",
    "        base_model.fit(x_train, y_train)\n",
    "        calibrated_model = CalibratedClassifierCV(base_model, cv='prefit')\n",
    "        calibrated_model.fit(x_train, y_train)\n",
    "        y_prob = calibrated_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "        # Thresholds to simulate\n",
    "        thresholds = [(0.5 + i * 0.05, 0.5 - i * 0.05) for i in range(1, 6)]\n",
    "\n",
    "        print(\"Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\")\n",
    "        print(\"-\" * 75)\n",
    "\n",
    "        for over_thresh, under_thresh in thresholds:\n",
    "            bets = []\n",
    "            actuals = []\n",
    "            odds_used = []\n",
    "\n",
    "            for prob, actual, o, u in zip(y_prob, y_test, over_test, under_test):\n",
    "                if prob >= over_thresh:\n",
    "                    bets.append(1)\n",
    "                    actuals.append(actual)\n",
    "                    odds_used.append(o)\n",
    "                elif prob <= under_thresh:\n",
    "                    bets.append(0)\n",
    "                    actuals.append(actual)\n",
    "                    odds_used.append(u)\n",
    "\n",
    "            if not bets:\n",
    "                continue\n",
    "\n",
    "            # Calculate profit per bet\n",
    "            profits = [compute_profit(p, a, odd) for p, a, odd in zip(bets, actuals, odds_used)]\n",
    "            total_profit = sum(profits)\n",
    "            total_risk = sum(abs(odd) if p != a else 100 for p, a, odd in zip(bets, actuals, odds_used))  # optional\n",
    "            roi = (total_profit / total_risk) * 100 if total_risk > 0 else 0\n",
    "            accuracy = sum([p == a for p, a in zip(bets, actuals)]) / len(bets)\n",
    "\n",
    "            print(f\"    {over_thresh:.2f}     |     {under_thresh:.2f}     |    {len(bets):4}     |  {accuracy:.4f} |   {total_profit:6.2f}  |  {roi:6.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Passthrough transformer that keeps DataFrame structure\n",
    "# Identity transformer to preserve DataFrame structure in pipeline\n",
    "preserve_df = FunctionTransformer(lambda x: x)\n",
    "\n",
    "# Best-performing models wrapped in pipelines\n",
    "best_models = {\n",
    "    'pts': make_pipeline(preserve_df, RandomForestClassifier(n_estimators=150, max_depth=8, random_state=42)),\n",
    "    'ast': make_pipeline(preserve_df, LogisticRegression(max_iter=1000)),\n",
    "    'reb': make_pipeline(preserve_df, LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42)),\n",
    "    '3pm': make_pipeline(preserve_df, RandomForestClassifier(n_estimators=150, max_depth=8, random_state=42)),\n",
    "}\n",
    "\n",
    "# Thresholds based on tuning\n",
    "category_thresholds = {\n",
    "    'pts': {'threshold_over': 0.65, 'threshold_under': 0.35},\n",
    "    'ast': {'threshold_over': 0.60, 'threshold_under': 0.40},\n",
    "    'reb': {'threshold_over': 0.75, 'threshold_under': 0.25},\n",
    "    '3pm': {'threshold_over': 0.65, 'threshold_under': 0.35},\n",
    "}\n",
    "\n",
    "saved_models = {}\n",
    "\n",
    "# Train and save each model\n",
    "for category, model in best_models.items():\n",
    "    df = full_data[category].copy()\n",
    "    df = df.dropna(subset=['result'])\n",
    "    df['result'] = df['result'].map(label_map) if df['result'].dtype == object else df['result']\n",
    "\n",
    "    x = df[classification_features[category]].fillna(0)\n",
    "    y = df['result']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    calibrated_model = CalibratedClassifierCV(model, cv='prefit')\n",
    "    calibrated_model.fit(x_train, y_train)\n",
    "\n",
    "    # Store calibrated model + the exact features used\n",
    "    saved_models[category] = {\n",
    "        'model': calibrated_model,\n",
    "        'threshold_over': category_thresholds[category]['threshold_over'],\n",
    "        'threshold_under': category_thresholds[category]['threshold_under'],\n",
    "        'features': list(x_train.columns)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Latest date used in regression training (data_ordered): 2023-04-09\n",
      "\n",
      "🚨 Checking for potential overlap between regression training and classification test sets:\n",
      "\n",
      "✅ PTS: Clean split — Regression ends 2023-04-09, classification test starts 2025-03-03\n",
      "✅ AST: Clean split — Regression ends 2023-04-09, classification test starts 2025-03-03\n",
      "✅ REB: Clean split — Regression ends 2023-04-09, classification test starts 2025-03-03\n",
      "✅ 3PM: Clean split — Regression ends 2023-04-09, classification test starts 2025-03-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68938/293419545.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['game_date'] = pd.to_datetime(train_data['game_date'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Categories\n",
    "categories = ['pts', 'ast', 'reb', '3pm']\n",
    "\n",
    "# --- STEP 1: Get Latest Regression Date from `data_ordered`\n",
    "train_data['game_date'] = pd.to_datetime(train_data['game_date'])\n",
    "regression_cutoff_date = train_data['game_date'].max()\n",
    "\n",
    "print(f\"Latest date used in regression training (data_ordered): {regression_cutoff_date.date()}\")\n",
    "\n",
    "# --- STEP 2: Get Earliest Test Date per Category\n",
    "test_start_dates = {}\n",
    "\n",
    "for category in categories:\n",
    "    df = full_data[category].copy()\n",
    "    df = df.dropna(subset=['result'])\n",
    "    df = df.sort_values('game_date')  # Just in case\n",
    "\n",
    "    x = df[classification_features[category]].fillna(0)\n",
    "    y = df['result']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    test_dates = df.loc[x_test.index, 'game_date']\n",
    "    test_start_dates[category] = test_dates.min()\n",
    "\n",
    "# --- STEP 3: Check for Overlap\n",
    "print(\"\\nChecking for potential overlap between regression training and classification test sets:\\n\")\n",
    "for cat in categories:\n",
    "    test_date = test_start_dates[cat]\n",
    "    if regression_cutoff_date >= test_date:\n",
    "        print(f\" {cat.upper()}: OVERLAP detected — Regression trained up to {regression_cutoff_date.date()}, classification test starts {test_date.date()}\")\n",
    "    else:\n",
    "        print(f\"{cat.upper()}: Clean split — Regression ends {regression_cutoff_date.date()}, classification test starts {test_date.date()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
