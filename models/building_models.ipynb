{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network._multilayer_perceptron import MLPRegressor\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime as date\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import TimeSeriesSplit \n",
    "from sklearn.metrics import r2_score,make_scorer,mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#using shifted windows for rolling data to prevent data leakage\n",
    "player_query = f\"\"\" \n",
    "SELECT *\n",
    "from `capstone_data.player_modeling_data_partitioned`\n",
    "order by game_date asc\n",
    "\"\"\"\n",
    "\n",
    "team_query = f\"\"\"\n",
    "SELECT *\n",
    "from `capstone_data.team_modeling_data_partitioned`\n",
    "order by game_date asc\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "try:\n",
    "    full_data = pd.read_csv('full_data.csv')\n",
    "\n",
    "except:\n",
    "    nba_player_data = pd.DataFrame(pandas_gbq.read_gbq(player_query,project_id='miscellaneous-projects-444203',progress_bar_type='tqdm'))\n",
    "    team_data = pd.DataFrame(pandas_gbq.read_gbq(team_query,project_id='miscellaneous-projects-444203',progress_bar_type='tqdm'))\n",
    "    team_data  = team_data.merge(team_data,on='game_id',suffixes=('',\"_opponent\"))\n",
    "    team_data = team_data[team_data[\"team_id\"] != team_data[\"team_id_opponent\"]]\n",
    "    full_data = nba_player_data.merge(team_data, on = ['game_id','team'], how = 'inner',suffixes=('','remove'))\n",
    "    full_data.drop([column for column in full_data.columns if 'remove' in column],axis = 1 , inplace=True) \n",
    "    full_data.drop([column for column in full_data.columns if '_1' in column],axis = 1 , inplace=True)\n",
    "    full_data.to_csv('full_data.csv',mode = 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered = full_data.sort_values('game_date')\n",
    "\n",
    "data_ordered.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Ideas \n",
    "\n",
    "* (ratio of 3pa and fga and 3pm and 3pa) TS% for players efg% \n",
    "* for players assist_to_turnover ratio assist ratio, \n",
    "* rebound_cahnce, defesnive reb %, \n",
    "* ast_ratio_season * pace, \n",
    "* home * pts season - data pts 3pm avg,\n",
    "* cold_streak pts_3gm_avg < pts_season boolean, \n",
    "* away difficulty away * opponent_defrtg_3gm_avg,\n",
    "* home_performance = data_ordered[data_ordered[\"home\"] == 1].groupby(\"team\")[\"pts_season\"].mean()\n",
    "* away_performance = data_ordered[data_ordered[\"away\"] == 1].groupby(\"team\")[\"pts_season\"].mean() these would be to see how the team performance changes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ordered['pts_per_min_3gm'] = data_ordered['pts_3gm_avg']/data_ordered['min_3gm_avg']\n",
    "# data_ordered['pts_per_min_season'] = data_ordered['pts_season']/data_ordered['min_season']\n",
    "# data_ordered['pts_per_min_momentum'] = data_ordered['pts_per_min_3gm'] - data_ordered['pts_per_min_season']\n",
    "\n",
    "# data_ordered['fg3m_per_min_3gm'] = data_ordered['fg3m_3gm_avg']/data_ordered['min_3gm_avg']\n",
    "# data_ordered['fg3m_per_min_season'] = data_ordered['fg3m_season']/data_ordered['min_season']\n",
    "# data_ordered['fg3m_per_min_momentum'] = data_ordered['fg3m_per_min_3gm'] - data_ordered['fg3m_per_min_season'] \n",
    "\n",
    "# data_ordered['reb_per_min_3gm'] = data_ordered['reb_3gm_avg']/data_ordered['min_3gm_avg']\n",
    "# data_ordered['reb_per_min_season'] = data_ordered['reb_season']/data_ordered['min_season']\n",
    "# data_ordered['reb_per_min_momentum'] = data_ordered['fg3m_per_min_3gm'] - data_ordered['reb_per_min_season']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_119416/3943372088.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data_ordered = data_ordered.groupby(['player','season']).apply(lambda x: x.iloc[3:]).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "data_ordered = data_ordered.groupby(['player','season']).apply(lambda x: x.iloc[3:]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered.sort_values(by='game_date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['game_date'] = pd.to_datetime(data_ordered['game_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['days_ago'] = (data_ordered['game_date'].max() - data_ordered['game_date']).dt.days\n",
    "data_ordered['time_decay_weight'] = 1 / (1 + np.log(1 + data_ordered['days_ago']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_ordered = data_ordered.drop('Unnamed: 0', axis =1)\n",
    "except KeyError:\n",
    "    print('Irregular column not made')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs with the column mean, but only for numeric columns\n",
    "data_ordered.fillna(data_ordered.select_dtypes(include=['number']).mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = data_ordered.select_dtypes(include=['number']).columns.tolist()\n",
    "numeric_columns = [column for column in numeric_columns if column not in ['pts','reb','ast','blk','stl','3pm','game_id','game_date','days_ago','time_decay_weight','team_id', \"gp_rank\", \"w_rank\", \"l_rank\", \"w_pct_rank\", \"min_rank\", \"fgm_rank\",\n",
    "    \"fga_rank\", \"fg_pct_rank\", \"fg3m_rank\", \"fg3a_rank\", \"fg3_pct_rank\",\n",
    "    \"ftm_rank\", \"fta_rank\", \"ft_pct_rank\", \"oreb_rank\", \"dreb_rank\",\n",
    "    \"reb_rank\", \"ast_rank\", \"tov_rank\", \"stl_rank\", \"blk_rank\",\n",
    "    \"blka_rank\", \"pf_rank\", \"pfd_rank\", \"pts_rank\", \"plus_minus_rank\",]]\n",
    "\n",
    "numeric_columns = [feature for feature in numeric_columns if any(keyword in feature for keyword in [\"3gm_avg\", \"season\", \"momentum\"])]\n",
    "features = {feature:[] for feature in ['pts','reb','ast','3pm']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['min_3gm_avg',\n",
       " 'min_season',\n",
       " 'min_momentum',\n",
       " 'fgm_3gm_avg',\n",
       " 'fgm_season',\n",
       " 'fgm_momentum',\n",
       " 'fga_3gm_avg',\n",
       " 'fga_season',\n",
       " 'fga_momentum',\n",
       " 'fg_pct_3gm_avg',\n",
       " 'fg_pct_season',\n",
       " 'fg_pct_momentum',\n",
       " 'fg3m_3gm_avg',\n",
       " 'fg3m_season',\n",
       " 'fg3m_momentum',\n",
       " 'fg3a_3gm_avg',\n",
       " 'fg3a_season',\n",
       " 'fg3a_momentum',\n",
       " 'fg3_pct_3gm_avg',\n",
       " 'fg3_pct_season',\n",
       " 'fg3_pct_momentum',\n",
       " 'ftm_3gm_avg',\n",
       " 'ftm_season',\n",
       " 'ftm_momentum',\n",
       " 'fta_3gm_avg',\n",
       " 'fta_season',\n",
       " 'fta_momentum',\n",
       " 'ft_pct_3gm_avg',\n",
       " 'ft_pct_season',\n",
       " 'ft_pct_momentum',\n",
       " 'oreb_3gm_avg',\n",
       " 'oreb_season',\n",
       " 'oreb_momentum',\n",
       " 'dreb_3gm_avg',\n",
       " 'dreb_season',\n",
       " 'dreb_momentum',\n",
       " 'reb_3gm_avg',\n",
       " 'reb_season',\n",
       " 'reb_momentum',\n",
       " 'ast_3gm_avg',\n",
       " 'ast_season',\n",
       " 'ast_momentum',\n",
       " 'stl_3gm_avg',\n",
       " 'stl_season',\n",
       " 'stl_momentum',\n",
       " 'blk_3gm_avg',\n",
       " 'blk_season',\n",
       " 'blk_momentum',\n",
       " 'to_3gm_avg',\n",
       " 'to_season',\n",
       " 'to_momentum',\n",
       " 'pf_3gm_avg',\n",
       " 'pf_season',\n",
       " 'pf_momentum',\n",
       " 'pts_3gm_avg',\n",
       " 'pts_season',\n",
       " 'pts_momentum',\n",
       " 'plus_minus_3gm_avg',\n",
       " 'plus_minus_season',\n",
       " 'plus_minus_momentum',\n",
       " 'season_start_year',\n",
       " 'tov_3gm_avg',\n",
       " 'tov_season',\n",
       " 'tov_momentum',\n",
       " 'blka_3gm_avg',\n",
       " 'blka_season',\n",
       " 'blka_momentum',\n",
       " 'pfd_3gm_avg',\n",
       " 'pfd_season',\n",
       " 'pfd_momentum',\n",
       " 'gp_rank_3gm_avg',\n",
       " 'gp_rank_season',\n",
       " 'gp_rank_momentum',\n",
       " 'w_rank_3gm_avg',\n",
       " 'w_rank_season',\n",
       " 'w_rank_momentum',\n",
       " 'l_rank_3gm_avg',\n",
       " 'l_rank_season',\n",
       " 'l_rank_momentum',\n",
       " 'w_pct_rank_3gm_avg',\n",
       " 'w_pct_rank_season',\n",
       " 'w_pct_rank_momentum',\n",
       " 'min_rank_3gm_avg',\n",
       " 'min_rank_season',\n",
       " 'min_rank_momentum',\n",
       " 'fgm_rank_3gm_avg',\n",
       " 'fgm_rank_season',\n",
       " 'fgm_rank_momentum',\n",
       " 'fga_rank_3gm_avg',\n",
       " 'fga_rank_season',\n",
       " 'fga_rank_momentum',\n",
       " 'fg_pct_rank_3gm_avg',\n",
       " 'fg_pct_rank_season',\n",
       " 'fg_pct_rank_momentum',\n",
       " 'fg3m_rank_3gm_avg',\n",
       " 'fg3m_rank_season',\n",
       " 'fg3m_rank_momentum',\n",
       " 'fg3a_rank_3gm_avg',\n",
       " 'fg3a_rank_season',\n",
       " 'fg3a_rank_momentum',\n",
       " 'fg3_pct_rank_3gm_avg',\n",
       " 'fg3_pct_rank_season',\n",
       " 'fg3_pct_rank_momentum',\n",
       " 'ftm_rank_3gm_avg',\n",
       " 'ftm_rank_season',\n",
       " 'ftm_rank_momentum',\n",
       " 'fta_rank_3gm_avg',\n",
       " 'fta_rank_season',\n",
       " 'fta_rank_momentum',\n",
       " 'ft_pct_rank_3gm_avg',\n",
       " 'ft_pct_rank_season',\n",
       " 'ft_pct_rank_momentum',\n",
       " 'oreb_rank_3gm_avg',\n",
       " 'oreb_rank_season',\n",
       " 'oreb_rank_momentum',\n",
       " 'dreb_rank_3gm_avg',\n",
       " 'dreb_rank_season',\n",
       " 'dreb_rank_momentum',\n",
       " 'reb_rank_3gm_avg',\n",
       " 'reb_rank_season',\n",
       " 'reb_rank_momentum',\n",
       " 'ast_rank_3gm_avg',\n",
       " 'ast_rank_season',\n",
       " 'ast_rank_momentum',\n",
       " 'tov_rank_3gm_avg',\n",
       " 'tov_rank_season',\n",
       " 'tov_rank_momentum',\n",
       " 'stl_rank_3gm_avg',\n",
       " 'stl_rank_season',\n",
       " 'stl_rank_momentum',\n",
       " 'blk_rank_3gm_avg',\n",
       " 'blk_rank_season',\n",
       " 'blk_rank_momentum',\n",
       " 'blka_rank_3gm_avg',\n",
       " 'blka_rank_season',\n",
       " 'blka_rank_momentum',\n",
       " 'pf_rank_3gm_avg',\n",
       " 'pf_rank_season',\n",
       " 'pf_rank_momentum',\n",
       " 'pfd_rank_3gm_avg',\n",
       " 'pfd_rank_season',\n",
       " 'pfd_rank_momentum',\n",
       " 'pts_rank_3gm_avg',\n",
       " 'pts_rank_season',\n",
       " 'pts_rank_momentum',\n",
       " 'plus_minus_rank_3gm_avg',\n",
       " 'plus_minus_rank_season',\n",
       " 'plus_minus_rank_momentum',\n",
       " 'season_start_year_opponent',\n",
       " 'min_3gm_avg_opponent',\n",
       " 'min_season_opponent',\n",
       " 'min_momentum_opponent',\n",
       " 'fgm_3gm_avg_opponent',\n",
       " 'fgm_season_opponent',\n",
       " 'fgm_momentum_opponent',\n",
       " 'fga_3gm_avg_opponent',\n",
       " 'fga_season_opponent',\n",
       " 'fga_momentum_opponent',\n",
       " 'fg_pct_3gm_avg_opponent',\n",
       " 'fg_pct_season_opponent',\n",
       " 'fg_pct_momentum_opponent',\n",
       " 'fg3m_3gm_avg_opponent',\n",
       " 'fg3m_season_opponent',\n",
       " 'fg3m_momentum_opponent',\n",
       " 'fg3a_3gm_avg_opponent',\n",
       " 'fg3a_season_opponent',\n",
       " 'fg3a_momentum_opponent',\n",
       " 'fg3_pct_3gm_avg_opponent',\n",
       " 'fg3_pct_season_opponent',\n",
       " 'fg3_pct_momentum_opponent',\n",
       " 'ftm_3gm_avg_opponent',\n",
       " 'ftm_season_opponent',\n",
       " 'ftm_momentum_opponent',\n",
       " 'fta_3gm_avg_opponent',\n",
       " 'fta_season_opponent',\n",
       " 'fta_momentum_opponent',\n",
       " 'ft_pct_3gm_avg_opponent',\n",
       " 'ft_pct_season_opponent',\n",
       " 'ft_pct_momentum_opponent',\n",
       " 'oreb_3gm_avg_opponent',\n",
       " 'oreb_season_opponent',\n",
       " 'oreb_momentum_opponent',\n",
       " 'dreb_3gm_avg_opponent',\n",
       " 'dreb_season_opponent',\n",
       " 'dreb_momentum_opponent',\n",
       " 'reb_3gm_avg_opponent',\n",
       " 'reb_season_opponent',\n",
       " 'reb_momentum_opponent',\n",
       " 'ast_3gm_avg_opponent',\n",
       " 'ast_season_opponent',\n",
       " 'ast_momentum_opponent',\n",
       " 'tov_3gm_avg_opponent',\n",
       " 'tov_season_opponent',\n",
       " 'tov_momentum_opponent',\n",
       " 'stl_3gm_avg_opponent',\n",
       " 'stl_season_opponent',\n",
       " 'stl_momentum_opponent',\n",
       " 'blk_3gm_avg_opponent',\n",
       " 'blk_season_opponent',\n",
       " 'blk_momentum_opponent',\n",
       " 'blka_3gm_avg_opponent',\n",
       " 'blka_season_opponent',\n",
       " 'blka_momentum_opponent',\n",
       " 'pf_3gm_avg_opponent',\n",
       " 'pf_season_opponent',\n",
       " 'pf_momentum_opponent',\n",
       " 'pfd_3gm_avg_opponent',\n",
       " 'pfd_season_opponent',\n",
       " 'pfd_momentum_opponent',\n",
       " 'pts_3gm_avg_opponent',\n",
       " 'pts_season_opponent',\n",
       " 'pts_momentum_opponent',\n",
       " 'plus_minus_3gm_avg_opponent',\n",
       " 'plus_minus_season_opponent',\n",
       " 'plus_minus_momentum_opponent',\n",
       " 'gp_rank_3gm_avg_opponent',\n",
       " 'gp_rank_season_opponent',\n",
       " 'gp_rank_momentum_opponent',\n",
       " 'w_rank_3gm_avg_opponent',\n",
       " 'w_rank_season_opponent',\n",
       " 'w_rank_momentum_opponent',\n",
       " 'l_rank_3gm_avg_opponent',\n",
       " 'l_rank_season_opponent',\n",
       " 'l_rank_momentum_opponent',\n",
       " 'w_pct_rank_3gm_avg_opponent',\n",
       " 'w_pct_rank_season_opponent',\n",
       " 'w_pct_rank_momentum_opponent',\n",
       " 'min_rank_3gm_avg_opponent',\n",
       " 'min_rank_season_opponent',\n",
       " 'min_rank_momentum_opponent',\n",
       " 'fgm_rank_3gm_avg_opponent',\n",
       " 'fgm_rank_season_opponent',\n",
       " 'fgm_rank_momentum_opponent',\n",
       " 'fga_rank_3gm_avg_opponent',\n",
       " 'fga_rank_season_opponent',\n",
       " 'fga_rank_momentum_opponent',\n",
       " 'fg_pct_rank_3gm_avg_opponent',\n",
       " 'fg_pct_rank_season_opponent',\n",
       " 'fg_pct_rank_momentum_opponent',\n",
       " 'fg3m_rank_3gm_avg_opponent',\n",
       " 'fg3m_rank_season_opponent',\n",
       " 'fg3m_rank_momentum_opponent',\n",
       " 'fg3a_rank_3gm_avg_opponent',\n",
       " 'fg3a_rank_season_opponent',\n",
       " 'fg3a_rank_momentum_opponent',\n",
       " 'fg3_pct_rank_3gm_avg_opponent',\n",
       " 'fg3_pct_rank_season_opponent',\n",
       " 'fg3_pct_rank_momentum_opponent',\n",
       " 'ftm_rank_3gm_avg_opponent',\n",
       " 'ftm_rank_season_opponent',\n",
       " 'ftm_rank_momentum_opponent',\n",
       " 'fta_rank_3gm_avg_opponent',\n",
       " 'fta_rank_season_opponent',\n",
       " 'fta_rank_momentum_opponent',\n",
       " 'ft_pct_rank_3gm_avg_opponent',\n",
       " 'ft_pct_rank_season_opponent',\n",
       " 'ft_pct_rank_momentum_opponent',\n",
       " 'oreb_rank_3gm_avg_opponent',\n",
       " 'oreb_rank_season_opponent',\n",
       " 'oreb_rank_momentum_opponent',\n",
       " 'dreb_rank_3gm_avg_opponent',\n",
       " 'dreb_rank_season_opponent',\n",
       " 'dreb_rank_momentum_opponent',\n",
       " 'reb_rank_3gm_avg_opponent',\n",
       " 'reb_rank_season_opponent',\n",
       " 'reb_rank_momentum_opponent',\n",
       " 'ast_rank_3gm_avg_opponent',\n",
       " 'ast_rank_season_opponent',\n",
       " 'ast_rank_momentum_opponent',\n",
       " 'tov_rank_3gm_avg_opponent',\n",
       " 'tov_rank_season_opponent',\n",
       " 'tov_rank_momentum_opponent',\n",
       " 'stl_rank_3gm_avg_opponent',\n",
       " 'stl_rank_season_opponent',\n",
       " 'stl_rank_momentum_opponent',\n",
       " 'blk_rank_3gm_avg_opponent',\n",
       " 'blk_rank_season_opponent',\n",
       " 'blk_rank_momentum_opponent',\n",
       " 'blka_rank_3gm_avg_opponent',\n",
       " 'blka_rank_season_opponent',\n",
       " 'blka_rank_momentum_opponent',\n",
       " 'pf_rank_3gm_avg_opponent',\n",
       " 'pf_rank_season_opponent',\n",
       " 'pf_rank_momentum_opponent',\n",
       " 'pfd_rank_3gm_avg_opponent',\n",
       " 'pfd_rank_season_opponent',\n",
       " 'pfd_rank_momentum_opponent',\n",
       " 'pts_rank_3gm_avg_opponent',\n",
       " 'pts_rank_season_opponent',\n",
       " 'pts_rank_momentum_opponent',\n",
       " 'plus_minus_rank_3gm_avg_opponent',\n",
       " 'plus_minus_rank_season_opponent',\n",
       " 'plus_minus_rank_momentum_opponent']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for category in features.keys():\n",
    "    x = train_data[numeric_columns]\n",
    "    y = train_data[category]\n",
    "\n",
    "    mi_scores = mutual_info_regression(x, y)\n",
    "    mi_scores = pd.Series(mi_scores, index=numeric_columns)\n",
    "    selected_features = mi_scores[mi_scores > 0.1].index.tolist()  # Keep features with MI > 0.05\n",
    "\n",
    "    features[category] = selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These values appeared to have non-linear relationships applying transformations\n",
    "# data_ordered['ft%_season'] = np.log1p(data_ordered['ft%_season'])\n",
    "# data_ordered['stl_3gm_avg'] = np.log1p(data_ordered['stl_3gm_avg'])\n",
    "# data_ordered['stl_season'] = np.log1p(data_ordered['stl_season'])\n",
    "# data_ordered['to_season'] = data_ordered['to_season']**2 \n",
    "# data_ordered['to_3gm_avg'] = data_ordered['to_3gm_avg']**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models = {category:{} for category in ['pts','reb','ast','3pm']} \n",
    "saved_results = {category:{} for category in ['pts','reb','ast','3pm']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP\n",
    "Applying shap to help reduce collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "pts\n",
      "0.5972425215458068\n",
      "120\n",
      "reb\n",
      "0.5331961310637111\n",
      "100\n",
      "ast\n",
      "0.5745602343485372\n",
      "92\n",
      "3pm\n",
      "0.40394212744145974\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for category in features.keys():\n",
    "\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    print(len(features_list))\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    linear_model = LinearRegression()\n",
    "\n",
    "    linear_model.fit(x_train,y_train)\n",
    "\n",
    "    y_pred = linear_model.predict(x_test)\n",
    "    print(category)\n",
    "    print(r2_score(y_true=y_test,y_pred=y_pred))\n",
    "\n",
    "    saved_results[category]['linear_model']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n",
    "    saved_models[category]['linear_model'] = linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts\n",
      "0.5972451814379129\n",
      "reb\n",
      "0.5332161552544693\n",
      "ast\n",
      "0.5745602379669869\n",
      "3pm\n",
      "0.403942153590657\n"
     ]
    }
   ],
   "source": [
    "for category in features.keys():\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    ridge_model = Ridge(alpha=1)\n",
    "\n",
    "    ridge_model.fit(x_train,y_train)\n",
    "\n",
    "    output = pd.DataFrame({'prediction':ridge_model.predict(x_test), 'actual':y_test})\n",
    "    print(category)\n",
    "    print(r2_score(y_true=output['actual'],y_pred=output['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# features = {}\n",
    "\n",
    "# for category in ['pts', 'reb', 'ast', '3pm']:\n",
    "#     x_train = train_data[numeric_columns]\n",
    "#     y_train = train_data[category]\n",
    "\n",
    "#     # Use Lasso to select features automatically\n",
    "#     lasso = Lasso(alpha=0.01)  # Adjust alpha based on tuning\n",
    "#     lasso.fit(x_train, y_train)\n",
    "\n",
    "#     # Keep only features with nonzero coefficients\n",
    "#     selected_features = [f for f, coef in zip(numeric_columns, lasso.coef_) if coef != 0]\n",
    "\n",
    "#     # Cap the number of features (e.g., max 50)\n",
    "#     selected_features = selected_features[:min(len(selected_features), 50)]\n",
    "\n",
    "#     features[category] = selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts: Cross-Val R² = 0.5725, Test R² = 0.5972\n",
      "reb: Cross-Val R² = 0.5316, Test R² = 0.5332\n",
      "ast: Cross-Val R² = 0.5818, Test R² = 0.5746\n",
      "3pm: Cross-Val R² = 0.4018, Test R² = 0.4039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for category in features.keys():\n",
    "    features_list = features[category]\n",
    "\n",
    "    x_train, y_train = train_data[features_list], train_data[category]\n",
    "    x_test, y_test = test_data[features_list], test_data[category]\n",
    "\n",
    "    linear_model = Ridge(alpha=1.0)  # Use Ridge instead of LinearRegression\n",
    "    linear_model.fit(x_train, y_train)\n",
    "\n",
    "    # Cross-validation score instead of just test R²\n",
    "    cv_r2 = cross_val_score(linear_model, x_train, y_train, cv=5, scoring='r2').mean()\n",
    "\n",
    "    y_pred = linear_model.predict(x_test)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{category}: Cross-Val R² = {cv_r2:.4f}, Test R² = {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(data_ordered[numeric_columns])\n",
    "\n",
    "scaled_data_df = pd.DataFrame(scaled_data,columns=numeric_columns)\n",
    "\n",
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "scaled_train_data = scaled_data_df.iloc[:split_index]\n",
    "scaled_test_data = scaled_data_df[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth':[3,6,9],'learning_rate':[.01,.05,.1,.3],'booster':['gbtree','dart'],'subsample':[.5,.7,.9],'colsample_bytree':[.5,.7,.9],'n_estimators':[100,300,500]}\n",
    "param_linear = {'booster':['gblinear'],'lambda':[0,.1,1,10],'alpha':[0,.1,1,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor = xgboost.XGBRegressor()\n",
    "mse_score = make_scorer(mean_squared_error,greater_is_better=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "scoring = {'MSE':mse_score,'r2':r2_scorer}\n",
    "grid_search = GridSearchCV(estimator=xgb_regressor,param_grid=param_grid,scoring = scoring,cv=tscv,n_jobs=1,verbose=0,refit='r2')\n",
    "grid_linear_search = GridSearchCV(estimator=xgb_regressor,param_grid=param_linear,scoring = scoring,cv=tscv,n_jobs=3,verbose=0,refit='r2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_features =  features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['min_3gm_avg',\n",
       " 'min_season',\n",
       " 'fgm_3gm_avg',\n",
       " 'fgm_season',\n",
       " 'fg_pct_season',\n",
       " 'fta_season',\n",
       " 'oreb_3gm_avg',\n",
       " 'oreb_season',\n",
       " 'dreb_3gm_avg',\n",
       " 'dreb_season',\n",
       " 'reb_3gm_avg',\n",
       " 'reb_season',\n",
       " 'blk_season',\n",
       " 'pf_season',\n",
       " 'pts_3gm_avg',\n",
       " 'pts_season',\n",
       " 'fgm_rank_season',\n",
       " 'fgm_rank_momentum',\n",
       " 'fga_rank_3gm_avg',\n",
       " 'fga_rank_season',\n",
       " 'fga_rank_momentum',\n",
       " 'fg_pct_rank_3gm_avg',\n",
       " 'fg_pct_rank_season',\n",
       " 'fg_pct_rank_momentum',\n",
       " 'fg3m_rank_season',\n",
       " 'fg3m_rank_momentum',\n",
       " 'fg3a_rank_3gm_avg',\n",
       " 'fg3a_rank_season',\n",
       " 'fg3a_rank_momentum',\n",
       " 'fg3_pct_rank_3gm_avg',\n",
       " 'fg3_pct_rank_season',\n",
       " 'fg3_pct_rank_momentum',\n",
       " 'ftm_rank_3gm_avg',\n",
       " 'ftm_rank_season',\n",
       " 'ftm_rank_momentum',\n",
       " 'fta_rank_3gm_avg',\n",
       " 'fta_rank_season',\n",
       " 'fta_rank_momentum',\n",
       " 'ft_pct_rank_3gm_avg',\n",
       " 'ft_pct_rank_season',\n",
       " 'ft_pct_rank_momentum',\n",
       " 'oreb_rank_season',\n",
       " 'oreb_rank_momentum',\n",
       " 'dreb_rank_season',\n",
       " 'dreb_rank_momentum',\n",
       " 'reb_rank_3gm_avg',\n",
       " 'reb_rank_season',\n",
       " 'reb_rank_momentum',\n",
       " 'ast_rank_season',\n",
       " 'ast_rank_momentum',\n",
       " 'tov_rank_season',\n",
       " 'tov_rank_momentum',\n",
       " 'stl_rank_season',\n",
       " 'stl_rank_momentum',\n",
       " 'blk_rank_season',\n",
       " 'blk_rank_momentum',\n",
       " 'blka_rank_season',\n",
       " 'blka_rank_momentum',\n",
       " 'pf_rank_season',\n",
       " 'pf_rank_momentum',\n",
       " 'pfd_rank_season',\n",
       " 'pfd_rank_momentum',\n",
       " 'pts_rank_3gm_avg',\n",
       " 'pts_rank_season',\n",
       " 'pts_rank_momentum',\n",
       " 'plus_minus_rank_3gm_avg',\n",
       " 'plus_minus_rank_season',\n",
       " 'plus_minus_rank_momentum',\n",
       " 'fgm_rank_season_opponent',\n",
       " 'fgm_rank_momentum_opponent',\n",
       " 'fga_rank_3gm_avg_opponent',\n",
       " 'fga_rank_season_opponent',\n",
       " 'fga_rank_momentum_opponent',\n",
       " 'fg_pct_rank_3gm_avg_opponent',\n",
       " 'fg_pct_rank_season_opponent',\n",
       " 'fg_pct_rank_momentum_opponent',\n",
       " 'fg3m_rank_season_opponent',\n",
       " 'fg3m_rank_momentum_opponent',\n",
       " 'fg3a_rank_3gm_avg_opponent',\n",
       " 'fg3a_rank_season_opponent',\n",
       " 'fg3a_rank_momentum_opponent',\n",
       " 'fg3_pct_rank_3gm_avg_opponent',\n",
       " 'fg3_pct_rank_season_opponent',\n",
       " 'fg3_pct_rank_momentum_opponent',\n",
       " 'ftm_rank_3gm_avg_opponent',\n",
       " 'ftm_rank_season_opponent',\n",
       " 'ftm_rank_momentum_opponent',\n",
       " 'fta_rank_3gm_avg_opponent',\n",
       " 'fta_rank_season_opponent',\n",
       " 'fta_rank_momentum_opponent',\n",
       " 'ft_pct_rank_3gm_avg_opponent',\n",
       " 'ft_pct_rank_season_opponent',\n",
       " 'ft_pct_rank_momentum_opponent',\n",
       " 'oreb_rank_season_opponent',\n",
       " 'oreb_rank_momentum_opponent',\n",
       " 'dreb_rank_season_opponent',\n",
       " 'dreb_rank_momentum_opponent',\n",
       " 'reb_rank_3gm_avg_opponent',\n",
       " 'reb_rank_season_opponent',\n",
       " 'reb_rank_momentum_opponent',\n",
       " 'ast_rank_season_opponent',\n",
       " 'ast_rank_momentum_opponent',\n",
       " 'tov_rank_season_opponent',\n",
       " 'tov_rank_momentum_opponent',\n",
       " 'stl_rank_season_opponent',\n",
       " 'stl_rank_momentum_opponent',\n",
       " 'blk_rank_season_opponent',\n",
       " 'blk_rank_momentum_opponent',\n",
       " 'blka_rank_season_opponent',\n",
       " 'blka_rank_momentum_opponent',\n",
       " 'pf_rank_season_opponent',\n",
       " 'pf_rank_momentum_opponent',\n",
       " 'pfd_rank_season_opponent',\n",
       " 'pfd_rank_momentum_opponent',\n",
       " 'pts_rank_3gm_avg_opponent',\n",
       " 'pts_rank_season_opponent',\n",
       " 'pts_rank_momentum_opponent',\n",
       " 'plus_minus_rank_3gm_avg_opponent',\n",
       " 'plus_minus_rank_season_opponent',\n",
       " 'plus_minus_rank_momentum_opponent']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_features['reb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts\n",
      "{'alpha': 0, 'booster': 'gblinear', 'lambda': 0}\n",
      "0.5746073167783181\n",
      "reb\n",
      "{'alpha': 0, 'booster': 'gblinear', 'lambda': 0}\n",
      "0.5322043614336138\n",
      "ast\n",
      "{'alpha': 0, 'booster': 'gblinear', 'lambda': 0}\n",
      "0.5796047472616144\n",
      "3pm\n",
      "{'alpha': 0, 'booster': 'gblinear', 'lambda': 0}\n",
      "0.3997743089742949\n"
     ]
    }
   ],
   "source": [
    "for category in features.keys():\n",
    "    x_train,y_train = scaled_train_data[xg_features[category]],train_data[category]\n",
    "    x_test, y_test = scaled_test_data[xg_features[category]],test_data[category]\n",
    "\n",
    "    fit_params = {'eval_set':[(x_test,y_test)],'early_stopping_rounds':20,'verbose':False}\n",
    "\n",
    "    grid_linear_search.estimator.set_params(eval_metric='rmse')\n",
    "\n",
    "\n",
    "    grid_linear_search.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "    print(category)\n",
    "    print(grid_linear_search.best_params_)\n",
    "    print(grid_linear_search.best_score_)\n",
    "\n",
    "    y_pred = grid_linear_search.best_estimator_.predict(x_test)\n",
    "\n",
    "    saved_models[category]['XGboost'] = grid_linear_search.best_estimator_\n",
    "    saved_results[category]['XGboost']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "light = lightgbm.LGBMRegressor(boosting_type='gbdt', n_estimators=500)  # Using hist for faster training\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50],  \n",
    "    'learning_rate': [0.01, 0.1],  \n",
    "    'max_depth': [-1, 10],  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_grid_search = GridSearchCV(estimator=light,param_grid=param_grid,cv=tscv,verbose=0,n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41312\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.752783\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 41378\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.903932\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41412\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.073039\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41416\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.218370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41411\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.270943\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41312\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.752783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41378\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.903932\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166170 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41412\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.073039\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41416\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.218370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.206971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41411\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.270943\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41312\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.752783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41378\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.903932\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41412\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.073039\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41416\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.218370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.247007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41411\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.270943\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41312\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.752783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41378\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.903932\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41412\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.073039\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41416\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.218370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215768 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41411\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.270943\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41312\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.752783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41378\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.903932\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41412\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.073039\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41416\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.218370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.214834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41411\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.270943\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41312\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.752783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41378\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.903932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41412\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.073039\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41416\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.218370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41411\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.270943\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41312\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.752783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41378\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.903932\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41412\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.073039\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41416\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.218370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41411\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.270943\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41312\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.752783\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 41378\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 9.903932\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41412\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.073039\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41416\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.218370\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41411\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.270943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41414\n",
      "[LightGBM] [Info] Number of data points in the train set: 386968, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 10.389676\n",
      "pts\n",
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': -1, 'num_leaves': 50}\n",
      "MSE: 30.827364605526707\n",
      "R2: 0.6135017686869715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29490\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.145818\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29520\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.123620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29534\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.159319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29540\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.188542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29536\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.191845\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29490\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.145818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29520\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.123620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29534\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.159319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29540\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.188542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29536\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.191845\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29490\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.145818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29520\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.123620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29534\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.159319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29540\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.188542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29536\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.191845\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29490\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.145818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29520\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.123620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29534\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.159319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29540\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.188542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29536\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.191845\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29490\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.145818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29520\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.123620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29534\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.159319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29540\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.188542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152768 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29536\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.191845\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065518 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29490\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.145818\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29520\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.123620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29534\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.159319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29540\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.188542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29536\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.191845\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051509 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29490\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.145818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29520\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.123620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29534\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.159319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29540\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.188542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29536\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.191845\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29490\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.145818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29520\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.123620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29534\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.159319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29540\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.188542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29536\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.191845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29538\n",
      "[LightGBM] [Info] Number of data points in the train set: 386968, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score 4.189031\n",
      "reb\n",
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'num_leaves': 31}\n",
      "MSE: 5.599741326148006\n",
      "R2: 0.535635523749175\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24425\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.107879\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24458\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.136055\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24470\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.191854\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24473\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.229293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24466\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.248693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24425\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.107879\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24458\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.136055\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24470\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.191854\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24473\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.229293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24466\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.248693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24425\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.107879\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24458\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.136055\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24470\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.191854\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24473\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.229293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24466\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.248693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24425\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.107879\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24458\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.136055\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136900 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24470\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.191854\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24473\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.229293\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24466\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.248693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24425\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.107879\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24458\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.136055\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24470\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.191854\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24473\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.229293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24466\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.248693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24425\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.107879\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24458\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.136055\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24470\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.191854\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24473\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.229293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24466\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.248693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058666 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24425\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.107879\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24458\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.136055\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24470\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.191854\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24473\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.229293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24466\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.248693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24425\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.107879\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24458\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.136055\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24470\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.191854\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24473\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.229293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24466\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.248693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24471\n",
      "[LightGBM] [Info] Number of data points in the train set: 386968, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 2.278346\n",
      "ast\n",
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'num_leaves': 31}\n",
      "MSE: 2.9915439188610473\n",
      "R2: 0.5764292641649603\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22559\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.826010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22583\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.889691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22603\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.942203\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22608\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.999965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22604\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 1.036434\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22559\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.826010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22583\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.889691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22603\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.942203\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22608\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.999965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22604\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 1.036434\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22559\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.826010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22583\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.889691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22603\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.942203\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22608\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.999965\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22604\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 1.036434\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22559\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.826010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074001 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22583\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.889691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22603\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.942203\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22608\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.999965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22604\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 1.036434\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22559\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.826010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22583\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.889691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22603\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.942203\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110728 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22608\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.999965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22604\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 1.036434\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22559\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.826010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22583\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.889691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22603\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.942203\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22608\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.999965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22604\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 1.036434\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22559\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.826010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22583\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.889691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22603\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.942203\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22608\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.999965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22604\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 1.036434\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22559\n",
      "[LightGBM] [Info] Number of data points in the train set: 64498, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.826010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22583\n",
      "[LightGBM] [Info] Number of data points in the train set: 128992, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.889691\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22603\n",
      "[LightGBM] [Info] Number of data points in the train set: 193486, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.942203\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22608\n",
      "[LightGBM] [Info] Number of data points in the train set: 257980, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.999965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22604\n",
      "[LightGBM] [Info] Number of data points in the train set: 322474, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 1.036434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22607\n",
      "[LightGBM] [Info] Number of data points in the train set: 386968, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 1.062313\n",
      "3pm\n",
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': -1, 'num_leaves': 50}\n",
      "MSE: 1.2867866885051822\n",
      "R2: 0.43836569622782817\n"
     ]
    }
   ],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]\n",
    "for category in features.keys():\n",
    "    x_train,y_train = train_data[features[category]],train_data[category]\n",
    "    x_test,y_test = test_data[features[category]],test_data[category]\n",
    "\n",
    "    light_grid_search.fit(x_train,y_train)\n",
    "\n",
    "    best_model = light_grid_search.best_estimator_\n",
    "    print(category)\n",
    "    print(\"Best Parameters:\", light_grid_search.best_params_)\n",
    "\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "\n",
    "    saved_models[category]['lightgbm'] = best_model\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'R2: {r2}')\n",
    "\n",
    "    saved_results[category]['lightgbm']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(saved_models,'models.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_performance.txt', 'w') as file:\n",
    "    for category, models in saved_results.items():\n",
    "        file.write(f\"Category: {category}\\n\")\n",
    "        for model, metrics in models.items():\n",
    "            file.write(f\"  Model: {model}\\n\")\n",
    "            for metric, value in metrics.items():\n",
    "                file.write(f\"    {metric}: {value}\\n\")\n",
    "        file.write(\"\\n\")  # Newline between categories\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
