{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.linear_model._logistic import LogisticRegression\n",
    "from datetime import datetime as dt\n",
    "from sklearn.model_selection import TimeSeriesSplit \n",
    "from sklearn.metrics import r2_score,make_scorer,mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import os\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_player_name(name):\n",
    "    \"\"\"Standardizes player names by removing special characters and handling known name variations.\"\"\"\n",
    "    name = name.lower().strip()  # Convert to lowercase & remove extra spaces\n",
    "    name = name.replace(\".\", \"\")  # Remove periods\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove special characters (apostrophes, dashes, etc.) \n",
    "    # Known name changes (add more as needed)\n",
    "    name_corrections = {\n",
    "        \"alexandre sarr\": \"alex sarr\",\n",
    "        \"jimmy butler\": \"jimmy butler iii\",\n",
    "        \"nicolas claxton\": \"nic claxton\",\n",
    "        \"kenyon martin jr\": \"kj martin\",\n",
    "        \"carlton carrington\": \"bub carrington\",\n",
    "        \"ron holland ii\": \"ronald holland ii\",\n",
    "        'cameron thomas':'cam thomas'\n",
    "    }\n",
    "\n",
    "    # Apply corrections if the name exists in the dictionary\n",
    "    return name_corrections.get(name, name)  # Default to original name if no correction found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#using shifted windows for rolling data to prevent data leakage\n",
    "player_query = f\"\"\" \n",
    "SELECT *\n",
    "from `capstone_data.player_modeling_data_partitioned`\n",
    "order by game_date asc\n",
    "\"\"\"\n",
    "\n",
    "team_query = f\"\"\"\n",
    "SELECT *\n",
    "from `capstone_data.team_modeling_data_partitioned`\n",
    "order by game_date asc\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    " \n",
    "try:\n",
    "    full_data = pd.read_csv('full_data.csv')\n",
    "\n",
    "except:\n",
    "    nba_player_data = pd.DataFrame(pandas_gbq.read_gbq(player_query,project_id='miscellaneous-projects-444203',progress_bar_type='tqdm'))\n",
    "    team_data = pd.DataFrame(pandas_gbq.read_gbq(team_query,project_id='miscellaneous-projects-444203',progress_bar_type='tqdm'))\n",
    "    team_data  = team_data.merge(team_data,on='game_id',suffixes=('',\"_opponent\"))\n",
    "    team_data = team_data[team_data[\"team_id\"] != team_data[\"team_id_opponent\"]]\n",
    "    full_data = nba_player_data.merge(team_data, on = ['game_id','team'], how = 'inner',suffixes=('','remove'))\n",
    "    full_data.drop([column for column in full_data.columns if 'remove' in column],axis = 1 , inplace=True) \n",
    "    full_data.drop([column for column in full_data.columns if '_1' in column],axis = 1 , inplace=True)\n",
    "    full_data.to_csv('full_data.csv',mode = 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered = full_data.sort_values('game_date')\n",
    "\n",
    "data_ordered.dropna(inplace=True,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Ideas \n",
    "\n",
    "* (ratio of 3pa and fga and 3pm and 3pa) TS% for players efg% \n",
    "* for players assist_to_turnover ratio assist ratio, \n",
    "* rebound_cahnce, defesnive reb %, \n",
    "* ast_ratio_season * pace, \n",
    "* home * pts season - data pts 3pm avg,\n",
    "* cold_streak pts_3gm_avg < pts_season boolean, \n",
    "* away difficulty away * opponent_defrtg_3gm_avg,\n",
    "* home_performance = data_ordered[data_ordered[\"home\"] == 1].groupby(\"team\")[\"pts_season\"].mean()\n",
    "* away_performance = data_ordered[data_ordered[\"away\"] == 1].groupby(\"team\")[\"pts_season\"].mean() these would be to see how the team performance changes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10837/3943372088.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data_ordered = data_ordered.groupby(['player','season']).apply(lambda x: x.iloc[3:]).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "data_ordered = data_ordered.groupby(['player','season']).apply(lambda x: x.iloc[3:]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered.sort_values(by='game_date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['game_date'] = pd.to_datetime(data_ordered['game_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['days_ago'] = (data_ordered['game_date'].max() - data_ordered['game_date']).dt.days\n",
    "data_ordered['time_decay_weight'] = 1 / (1 + np.log(1 + data_ordered['days_ago']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irregular column not made\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data_ordered = data_ordered.drop('Unnamed: 0', axis =1)\n",
    "except KeyError:\n",
    "    print('Irregular column not made')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs with the column mean, but only for numeric columns\n",
    "data_ordered.fillna(data_ordered.select_dtypes(include=['number']).mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = data_ordered.select_dtypes(include=['number']).columns.tolist()\n",
    "numeric_columns = [column for column in numeric_columns if column not in ['pts','reb','ast','blk','stl','3pm','game_id','game_date','days_ago','time_decay_weight','team_id', \"gp_rank\", \"w_rank\", \"l_rank\", \"w_pct_rank\", \"min_rank\", \"fgm_rank\",\n",
    "    \"fga_rank\", \"fg_pct_rank\", \"fg3m_rank\", \"fg3a_rank\", \"fg3_pct_rank\",\n",
    "    \"ftm_rank\", \"fta_rank\", \"ft_pct_rank\", \"oreb_rank\", \"dreb_rank\",\n",
    "    \"reb_rank\", \"ast_rank\", \"tov_rank\", \"stl_rank\", \"blk_rank\",\n",
    "    \"blka_rank\", \"pf_rank\", \"pfd_rank\", \"pts_rank\", \"plus_minus_rank\",]]\n",
    "\n",
    "numeric_columns = [feature for feature in numeric_columns if any(keyword in feature for keyword in [\"3gm_avg\", \"season\", \"momentum\"])]\n",
    "features = {feature:[] for feature in ['pts','reb','ast','3pm']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for category in features.keys():\n",
    "    x = train_data[numeric_columns]\n",
    "    y = train_data[category]\n",
    "\n",
    "    mi_scores = mutual_info_regression(x, y)\n",
    "    mi_scores = pd.Series(mi_scores, index=numeric_columns)\n",
    "    selected_features = mi_scores[mi_scores > 0.10].index.tolist()  \n",
    "\n",
    "    features[category] = selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models = {category:{} for category in ['pts','reb','ast','3pm']} \n",
    "saved_results = {category:{} for category in ['pts','reb','ast','3pm']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP\n",
    "Applying shap to help reduce collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "pts\n",
      "0.5948915824979621\n",
      "16\n",
      "reb\n",
      "0.5274784843529623\n",
      "16\n",
      "ast\n",
      "0.5683473929114774\n",
      "8\n",
      "3pm\n",
      "0.3961275854319022\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for category in features.keys():\n",
    "\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    print(len(features_list))\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    linear_model = LinearRegression()\n",
    "\n",
    "    linear_model.fit(x_train,y_train)\n",
    "\n",
    "    y_pred = linear_model.predict(x_test)\n",
    "    print(category)\n",
    "    print(r2_score(y_true=y_test,y_pred=y_pred))\n",
    "\n",
    "    saved_results[category]['linear_model']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n",
    "    saved_models[category]['linear_model'] = linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts\n",
      "0.594892200502906\n",
      "reb\n",
      "0.5275028533316133\n",
      "ast\n",
      "0.5683473938529809\n",
      "3pm\n",
      "0.39612756225732126\n"
     ]
    }
   ],
   "source": [
    "for category in features.keys():\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    ridge_model = Ridge(alpha=1)\n",
    "\n",
    "    ridge_model.fit(x_train,y_train)\n",
    "\n",
    "    output = pd.DataFrame({'prediction':ridge_model.predict(x_test), 'actual':y_test})\n",
    "    print(category)\n",
    "    print(r2_score(y_true=output['actual'],y_pred=output['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts: Cross-Val R² = 0.5704, Test R² = 0.5949\n",
      "reb: Cross-Val R² = 0.5280, Test R² = 0.5275\n",
      "ast: Cross-Val R² = 0.5794, Test R² = 0.5683\n",
      "3pm: Cross-Val R² = 0.3997, Test R² = 0.3961\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for category in features.keys():\n",
    "    features_list = features[category]\n",
    "\n",
    "    x_train, y_train = train_data[features_list], train_data[category]\n",
    "    x_test, y_test = test_data[features_list], test_data[category]\n",
    "\n",
    "    linear_model = Ridge(alpha=1.0)  # Use Ridge instead of LinearRegression\n",
    "    linear_model.fit(x_train, y_train)\n",
    "\n",
    "    # Cross-validation score instead of just test R²\n",
    "    cv_r2 = cross_val_score(linear_model, x_train, y_train, cv=5, scoring='r2').mean()\n",
    "\n",
    "    y_pred = linear_model.predict(x_test)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{category}: Cross-Val R² = {cv_r2:.4f}, Test R² = {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "lgb_model = LGBMRegressor(n_estimators=1000, random_state=42,verbosity=-1)\n",
    "\n",
    "# Define the expanded parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [15, 31, 50, 75],\n",
    "    'learning_rate': [0.005, 0.01, 0.05],\n",
    "    'max_depth': [-1, 5, 10, 15],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Time series split (if your data is chronological)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Randomized search setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=40,  # control number of total combinations to test\n",
    "    cv=tscv,\n",
    "    scoring='r2',\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit to your training data\n",
    "# Best model + params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m x_train,y_train \u001b[38;5;241m=\u001b[39m train_data[features[category]],train_data[category]\n\u001b[1;32m      7\u001b[0m x_test,y_test \u001b[38;5;241m=\u001b[39m test_data[features[category]],test_data[category]\n\u001b[0;32m----> 9\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m best_model \u001b[38;5;241m=\u001b[39m random_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(category)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1960\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1958\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1960\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]\n",
    "for category in features.keys():\n",
    "    x_train,y_train = train_data[features[category]],train_data[category]\n",
    "    x_test,y_test = test_data[features[category]],test_data[category]\n",
    "\n",
    "    random_search.fit(x_train,y_train)\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    print(category)\n",
    "    print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "\n",
    "    saved_models[category]['lightgbm'] = best_model\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'R2: {r2}')\n",
    "\n",
    "    saved_results[category]['lightgbm']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(saved_models,'models.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_performance.txt', 'w') as file:\n",
    "    for category, models in saved_results.items():\n",
    "        file.write(f\"Category: {category}\\n\")\n",
    "        for model, metrics in models.items():\n",
    "            file.write(f\"  Model: {model}\\n\")\n",
    "            for metric, value in metrics.items():\n",
    "                file.write(f\"    {metric}: {value}\\n\")\n",
    "        file.write(\"\\n\")  # Newline between categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Modeling into Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Ensemble modeling\n",
    "\n",
    "saved_models = joblib.load('models.pkl')\n",
    "\n",
    "linear_models = {cat: saved_models[cat]['linear_model'] for cat in saved_models if 'linear_model' in saved_models[cat]}\n",
    "lightgbm_models = {cat: saved_models[cat]['lightgbm'] for cat in saved_models if 'lightgbm' in saved_models[cat]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts Meta-model R²: 0.6164, MSE: 30.4233\n",
      "reb Meta-model R²: 0.5308, MSE: 5.6493\n",
      "ast Meta-model R²: 0.5695, MSE: 3.0275\n",
      "3pm Meta-model R²: 0.4488, MSE: 1.2632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "meta_models = {}\n",
    "meta_results = {}\n",
    "\n",
    "for category in ['pts', 'reb', 'ast', '3pm']:\n",
    "    # Load models\n",
    "    lm = saved_models[category]['linear_model']\n",
    "    lgb = saved_models[category]['lightgbm']\n",
    "\n",
    "    # Prepare test data\n",
    "    lm_features_list = [f.strip() for f in lm.feature_names_in_]\n",
    "    lgb_features_list = [f.strip() for f in lgb.feature_names_in_]\n",
    "\n",
    "    lm_x_test = test_data[lm_features_list]\n",
    "\n",
    "    lgb_x_test = test_data[lgb_features_list]\n",
    "\n",
    "    y_test = test_data[category]\n",
    "\n",
    "    # Get predictions\n",
    "    preds_lm = lm.predict(lm_x_test)\n",
    "    preds_lgb = lgb.predict(lgb_x_test)\n",
    "\n",
    "    # Stack predictions into meta-model features\n",
    "    meta_X = np.vstack([preds_lm, preds_lgb]).T\n",
    "    meta_y = y_test.values\n",
    "\n",
    "    # Train meta-model\n",
    "    meta_model = Ridge()\n",
    "    meta_model.fit(meta_X, meta_y)\n",
    "\n",
    "    # Evaluate meta-model\n",
    "    meta_preds = meta_model.predict(meta_X)\n",
    "    r2 = r2_score(meta_y, meta_preds)\n",
    "    mse = mean_squared_error(meta_y, meta_preds)\n",
    "\n",
    "    print(f\"{category} Meta-model R²: {r2:.4f}, MSE: {mse:.4f}\")\n",
    "    \n",
    "    meta_models[category] = meta_model\n",
    "    meta_results[category] = {'r2': r2, 'mse': mse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta_model.pkl']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(meta_models,'meta_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTS Meta-Model Weights:\n",
      "  Linear Model Weight:  -0.1503\n",
      "  LightGBM Weight:      1.1462\n",
      "REB Meta-Model Weights:\n",
      "  Linear Model Weight:  0.1157\n",
      "  LightGBM Weight:      0.8922\n",
      "AST Meta-Model Weights:\n",
      "  Linear Model Weight:  0.1427\n",
      "  LightGBM Weight:      0.8560\n",
      "3PM Meta-Model Weights:\n",
      "  Linear Model Weight:  -0.2429\n",
      "  LightGBM Weight:      1.2381\n"
     ]
    }
   ],
   "source": [
    "coef = {}\n",
    "for category, model in meta_models.items():\n",
    "    coef_linear, coef_lgbm = model.coef_\n",
    "    print(f\"{category.upper()} Meta-Model Weights:\")\n",
    "    print(f\"  Linear Model Weight:  {coef_linear:.4f}\")\n",
    "    print(f\"  LightGBM Weight:      {coef_lgbm:.4f}\")\n",
    "\n",
    "    coef[f'{category}_lm'] = coef_linear\n",
    "    coef[f'{category}_lgb'] = coef_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Model into Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_profit(pred, actual, odds):\n",
    "    if pred == actual:\n",
    "        return 100 if odds < 0 else odds\n",
    "    else:\n",
    "        return -abs(odds) if odds < 0 else -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "cats = ['points','assists','rebounds','threes_made']\n",
    "categories = ['pts','ast','reb','3pm']\n",
    "odds_data = {}\n",
    "for cat,category in zip(cats,categories):\n",
    "    predictions_query = f\"\"\"\n",
    "    select \n",
    "        po.*,\n",
    "        pp.Over,\n",
    "        pp.Under,\n",
    "        pp.{category}_linear_model,\n",
    "        pp.{category}_lightgbm\n",
    "    from `capstone_data.{cat}_predictions` pp\n",
    "    inner join `capstone_data.{category}_outcome` po\n",
    "        on pp.Player = po.player and date(pp.Date_Updated) = po.game_date\n",
    "    where pp.{category}_linear_model is not null\n",
    "    \"\"\" \n",
    "    data = pandas_gbq.read_gbq(predictions_query,project_id='miscellaneous-projects-444203')\n",
    "\n",
    "    \n",
    "    odds_data[category] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-03-28 00:00:00')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ordered['game_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = {category:[] for category in categories}\n",
    "\n",
    "data_ordered['player'] = data_ordered['player'].apply(clean_player_name)\n",
    "\n",
    "for category in categories:\n",
    "\n",
    "    data = odds_data[category]\n",
    "    \n",
    "    data['game_date'] = pd.to_datetime(data['game_date'])\n",
    "\n",
    "    data['player']= data['player'].apply(clean_player_name)\n",
    "\n",
    "    data = data.merge(data_ordered, on=['player','game_date'],how='inner')\n",
    "\n",
    "    data = data.drop_duplicates(subset=['player', 'game_date'], keep='first')\n",
    "\n",
    "    full_data[category] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat,category in zip(cats,categories):\n",
    "    full_data[category][f'{category}_ensemble'] = (pd.to_numeric(full_data[category][f'{category}_linear_model']) * \n",
    "    coef[f'{category}_lm'] + pd.to_numeric(full_data[category][f'{category}_lightgbm'] * coef[f'{category}_lgb']))\n",
    "\n",
    "    full_data[category][f'{category}_delta'] = full_data[category][f'{cat}'] - full_data[category][f'{category}_ensemble']\n",
    "\n",
    "    full_data[category].sort_values(by='game_date',inplace=True)\n",
    "\n",
    "    full_data[category].drop('time_decay_weight',axis = 1, inplace = True)\n",
    "    full_data[category].drop('days_ago',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player\n",
      "points\n",
      "pts_x\n",
      "game_date\n",
      "result\n",
      "recommendation_pts_linear_model\n",
      "recommendation_pts_lightgbm\n",
      "Over\n",
      "Under\n",
      "pts_linear_model\n",
      "pts_lightgbm\n",
      "game_id\n",
      "team_id\n",
      "team\n",
      "team_city\n",
      "player_id\n",
      "min\n",
      "fgm\n",
      "fga\n",
      "fg_pct\n",
      "3pm\n",
      "fg3a\n",
      "fg3_pct\n",
      "ftm\n",
      "fta\n",
      "ft_pct\n",
      "oreb\n",
      "dreb\n",
      "reb\n",
      "ast\n",
      "stl\n",
      "blk\n",
      "to\n",
      "pf\n",
      "pts_y\n",
      "plus_minus\n",
      "season\n",
      "min_3gm_avg\n",
      "min_season\n",
      "min_momentum\n",
      "fgm_3gm_avg\n",
      "fgm_season\n",
      "fgm_momentum\n",
      "fga_3gm_avg\n",
      "fga_season\n",
      "fga_momentum\n",
      "fg_pct_3gm_avg\n",
      "fg_pct_season\n",
      "fg_pct_momentum\n",
      "fg3m_3gm_avg\n",
      "fg3m_season\n",
      "fg3m_momentum\n",
      "fg3a_3gm_avg\n",
      "fg3a_season\n",
      "fg3a_momentum\n",
      "fg3_pct_3gm_avg\n",
      "fg3_pct_season\n",
      "fg3_pct_momentum\n",
      "ftm_3gm_avg\n",
      "ftm_season\n",
      "ftm_momentum\n",
      "fta_3gm_avg\n",
      "fta_season\n",
      "fta_momentum\n",
      "ft_pct_3gm_avg\n",
      "ft_pct_season\n",
      "ft_pct_momentum\n",
      "oreb_3gm_avg\n",
      "oreb_season\n",
      "oreb_momentum\n",
      "dreb_3gm_avg\n",
      "dreb_season\n",
      "dreb_momentum\n",
      "reb_3gm_avg\n",
      "reb_season\n",
      "reb_momentum\n",
      "ast_3gm_avg\n",
      "ast_season\n",
      "ast_momentum\n",
      "stl_3gm_avg\n",
      "stl_season\n",
      "stl_momentum\n",
      "blk_3gm_avg\n",
      "blk_season\n",
      "blk_momentum\n",
      "to_3gm_avg\n",
      "to_season\n",
      "to_momentum\n",
      "pf_3gm_avg\n",
      "pf_season\n",
      "pf_momentum\n",
      "pts_3gm_avg\n",
      "pts_season\n",
      "pts_momentum\n",
      "plus_minus_3gm_avg\n",
      "plus_minus_season\n",
      "plus_minus_momentum\n",
      "season_start_year\n",
      "season_id\n",
      "team_name\n",
      "matchup\n",
      "wl\n",
      "fg3m\n",
      "tov\n",
      "tov_3gm_avg\n",
      "tov_season\n",
      "tov_momentum\n",
      "season_id_opponent\n",
      "team_id_opponent\n",
      "team_opponent\n",
      "team_name_opponent\n",
      "game_date_opponent\n",
      "matchup_opponent\n",
      "wl_opponent\n",
      "min_opponent\n",
      "fgm_opponent\n",
      "fga_opponent\n",
      "fg_pct_opponent\n",
      "fg3m_opponent\n",
      "fg3a_opponent\n",
      "fg3_pct_opponent\n",
      "ftm_opponent\n",
      "fta_opponent\n",
      "ft_pct_opponent\n",
      "oreb_opponent\n",
      "dreb_opponent\n",
      "reb_opponent\n",
      "ast_opponent\n",
      "stl_opponent\n",
      "blk_opponent\n",
      "tov_opponent\n",
      "pf_opponent\n",
      "pts_opponent\n",
      "plus_minus_opponent\n",
      "season_start_year_opponent\n",
      "min_3gm_avg_opponent\n",
      "min_season_opponent\n",
      "min_momentum_opponent\n",
      "fgm_3gm_avg_opponent\n",
      "fgm_season_opponent\n",
      "fgm_momentum_opponent\n",
      "fga_3gm_avg_opponent\n",
      "fga_season_opponent\n",
      "fga_momentum_opponent\n",
      "fg_pct_3gm_avg_opponent\n",
      "fg_pct_season_opponent\n",
      "fg_pct_momentum_opponent\n",
      "fg3m_3gm_avg_opponent\n",
      "fg3m_season_opponent\n",
      "fg3m_momentum_opponent\n",
      "fg3a_3gm_avg_opponent\n",
      "fg3a_season_opponent\n",
      "fg3a_momentum_opponent\n",
      "fg3_pct_3gm_avg_opponent\n",
      "fg3_pct_season_opponent\n",
      "fg3_pct_momentum_opponent\n",
      "ftm_3gm_avg_opponent\n",
      "ftm_season_opponent\n",
      "ftm_momentum_opponent\n",
      "fta_3gm_avg_opponent\n",
      "fta_season_opponent\n",
      "fta_momentum_opponent\n",
      "ft_pct_3gm_avg_opponent\n",
      "ft_pct_season_opponent\n",
      "ft_pct_momentum_opponent\n",
      "oreb_3gm_avg_opponent\n",
      "oreb_season_opponent\n",
      "oreb_momentum_opponent\n",
      "dreb_3gm_avg_opponent\n",
      "dreb_season_opponent\n",
      "dreb_momentum_opponent\n",
      "reb_3gm_avg_opponent\n",
      "reb_season_opponent\n",
      "reb_momentum_opponent\n",
      "ast_3gm_avg_opponent\n",
      "ast_season_opponent\n",
      "ast_momentum_opponent\n",
      "stl_3gm_avg_opponent\n",
      "stl_season_opponent\n",
      "stl_momentum_opponent\n",
      "blk_3gm_avg_opponent\n",
      "blk_season_opponent\n",
      "blk_momentum_opponent\n",
      "tov_3gm_avg_opponent\n",
      "tov_season_opponent\n",
      "tov_momentum_opponent\n",
      "pf_3gm_avg_opponent\n",
      "pf_season_opponent\n",
      "pf_momentum_opponent\n",
      "pts_3gm_avg_opponent\n",
      "pts_season_opponent\n",
      "pts_momentum_opponent\n",
      "plus_minus_3gm_avg_opponent\n",
      "plus_minus_season_opponent\n",
      "plus_minus_momentum_opponent\n",
      "days_ago\n",
      "pts_ensemble\n",
      "pts_delta\n"
     ]
    }
   ],
   "source": [
    "for column in full_data['pts']:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing category: PTS ===\n",
      "Top averaged MI features for PTS:\n",
      "['plus_minus_momentum', 'pts_lightgbm', 'pts_momentum', 'fga_momentum', 'ftm_season', 'dreb_momentum', 'ft_pct_momentum', 'fg_pct_momentum', 'fta_opponent', 'fg3_pct_momentum', 'fg3a_season', 'pts_3gm_avg', 'fg3a_opponent', 'Over', 'pf_momentum']\n",
      "\n",
      "=== Processing category: AST ===\n",
      "Top averaged MI features for AST:\n",
      "['ftm_season', 'oreb_season', 'fg3a_season', 'ast_lightgbm', 'reb_momentum', 'dreb_momentum', 'oreb_momentum', 'fta_season_opponent', 'fg_pct_3gm_avg', 'Over', 'pts_season', 'min_3gm_avg', 'ast_3gm_avg', 'min_season', 'plus_minus_season']\n",
      "\n",
      "=== Processing category: REB ===\n",
      "Top averaged MI features for REB:\n",
      "['ftm_season', 'fta_momentum', 'blk_momentum', 'fg3m_season', 'reb_linear_model', 'fgm_opponent', 'fta_3gm_avg', 'fg_pct_season', 'ft_pct_momentum', 'fgm_season_opponent', 'oreb_3gm_avg', 'dreb_season', 'fg3a_season', 'min_momentum_opponent', 'fg3a_opponent']\n",
      "\n",
      "=== Processing category: 3PM ===\n",
      "Top averaged MI features for 3PM:\n",
      "['dreb_momentum', 'ast_momentum', 'ftm_season', 'fg3m', 'blk_season', 'fg_pct_season', 'ft_pct_3gm_avg', 'ftm_opponent', 'to_season', 'fg3_pct_momentum', 'to_3gm_avg', 'pf_season', 'ast_season', 'season_id_opponent', 'blk_momentum']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "classification_features = {}\n",
    "mi_feature_sets = {}\n",
    "categories = ['pts', 'ast', 'reb', '3pm']\n",
    "\n",
    "box_score_cols = [\n",
    "    'pts', 'pts_y', 'pts_x', 'ast_x', 'ast_y', 'reb_x', 'reb_y', 'reb', 'ast',\n",
    "    'stl', 'blk', 'to', 'pf', 'min', 'fgm', 'fga', 'fg_pct',\n",
    "    '3pm_y', 'fg3a', 'fg3_pct', 'ftm', 'fta', 'ft_pct',\n",
    "    'oreb', 'dreb', 'plus_minus', '3pm', '3pm_x'\n",
    "]\n",
    "exclude_cols = box_score_cols + ['result', 'player', 'game_date', 'team', 'team_city', 'matchup', 'matchup_opponent']\n",
    "\n",
    "NUM_RUNS = 10\n",
    "SEEDS = list(range(42, 42 + NUM_RUNS))\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\n=== Processing category: {category.upper()} ===\")\n",
    "    data = full_data[category].copy()\n",
    "\n",
    "    if isinstance(data['result'].iloc[0], str):\n",
    "        data['result'] = data['result'].map({'Over': 1, 'Under': 0})\n",
    "\n",
    "    data = data.dropna(subset=['result'])\n",
    "\n",
    "    # Define features\n",
    "    candidate_features = [col for col in data.columns if col not in exclude_cols]\n",
    "    numeric_features = [col for col in candidate_features if pd.api.types.is_numeric_dtype(data[col])]\n",
    "\n",
    "    X = data[numeric_features].fillna(0)\n",
    "    y = data['result']\n",
    "\n",
    "    # ➤ Split first (no shuffle to preserve game ordering if time-based)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    # ➤ Standard scale only on training set\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # ➤ MI on training data only\n",
    "    mi_matrix = []\n",
    "    for seed in SEEDS:\n",
    "        mi_scores = mutual_info_classif(X_train_scaled, y_train, random_state=seed)\n",
    "        mi_matrix.append(mi_scores)\n",
    "\n",
    "    mi_avg_scores = np.mean(mi_matrix, axis=0)\n",
    "    mi_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Avg_MI_Score': mi_avg_scores\n",
    "    }).sort_values(by='Avg_MI_Score', ascending=False)\n",
    "\n",
    "    # ➤ Select top 15 features\n",
    "    top_features = mi_df['Feature'].head(15).tolist()\n",
    "    mi_feature_sets[category] = top_features\n",
    "    classification_features[category] = top_features\n",
    "\n",
    "    print(f\"Top averaged MI features for {category.upper()}:\")\n",
    "    print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m label_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOver\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnder\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define model options\u001b[39;00m\n\u001b[1;32m      7\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mRandomForestClassifier\u001b[49m(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogisticRegression\u001b[39m\u001b[38;5;124m'\u001b[39m: LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m),\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM\u001b[39m\u001b[38;5;124m'\u001b[39m: LGBMClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStacked\u001b[39m\u001b[38;5;124m'\u001b[39m: StackingClassifier(\n\u001b[1;32m     12\u001b[0m         estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     13\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)),\n\u001b[1;32m     14\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgbm\u001b[39m\u001b[38;5;124m'\u001b[39m, LGBMClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)),\n\u001b[1;32m     15\u001b[0m         ],\n\u001b[1;32m     16\u001b[0m         final_estimator\u001b[38;5;241m=\u001b[39mLogisticRegression()\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m }\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Results storage\u001b[39;00m\n\u001b[1;32m     21\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "THRESH_OVER = 0.65\n",
    "THRESH_UNDER = 0.35\n",
    "label_map = {'Over': 1, 'Under': 0}\n",
    "\n",
    "# Define model options\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=150, max_depth=8, random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42),\n",
    "    'Stacked': StackingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=100, random_state=42)),\n",
    "        ],\n",
    "        final_estimator=LogisticRegression()\n",
    "    )\n",
    "}\n",
    "\n",
    "# Results storage\n",
    "results = {}\n",
    "# New: temp store to pick best threshold later\n",
    "category_results = []\n",
    "\n",
    "# Helper for American odds payout\n",
    "def compute_profit(pred, actual, odds):\n",
    "    if pred == actual:\n",
    "        return 100 if odds < 0 else odds\n",
    "    else:\n",
    "        return -abs(odds) if odds < 0 else -100\n",
    "\n",
    "# Loop through models and categories\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n=== Model: {model_name} ===\")\n",
    "    results[model_name] = {}\n",
    "    \n",
    "    for category in ['pts', 'ast', 'reb', '3pm']:\n",
    "        print(f\"\\n--- Category: {category.upper()} ---\")\n",
    "        df = full_data[category].copy()\n",
    "        df = df.dropna(subset=['result'])\n",
    "        df['result'] = df['result'].map(label_map) if df['result'].dtype == object else df['result']\n",
    "\n",
    "        x = df[classification_features[category]].fillna(0)\n",
    "        y = df['result']\n",
    "        over_odds = df['Over'].values\n",
    "        under_odds = df['Under'].values\n",
    "\n",
    "        # Train-test split\n",
    "        x_train, x_test, y_train, y_test, over_train, over_test, under_train, under_test = train_test_split(\n",
    "            x, y, over_odds, under_odds, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Fit + calibrate\n",
    "        base_model = model\n",
    "        base_model.fit(x_train, y_train)\n",
    "        calibrated_model = CalibratedClassifierCV(base_model, cv='prefit')\n",
    "        calibrated_model.fit(x_train, y_train)\n",
    "        y_prob = calibrated_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "        # Thresholds to simulate\n",
    "        thresholds = [(0.5 + i * 0.05, 0.5 - i * 0.05) for i in range(1, 6)]\n",
    "\n",
    "        print(\"Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\")\n",
    "        print(\"-\" * 75)\n",
    "\n",
    "        for over_thresh, under_thresh in thresholds:\n",
    "            bets = []\n",
    "            actuals = []\n",
    "            odds_used = []\n",
    "\n",
    "            for prob, actual, o, u in zip(y_prob, y_test, over_test, under_test):\n",
    "                if prob >= over_thresh:\n",
    "                    bets.append(1)\n",
    "                    actuals.append(actual)\n",
    "                    odds_used.append(o)\n",
    "                elif prob <= under_thresh:\n",
    "                    bets.append(0)\n",
    "                    actuals.append(actual)\n",
    "                    odds_used.append(u)\n",
    "\n",
    "            if not bets:\n",
    "                continue\n",
    "\n",
    "            # Calculate profit per bet\n",
    "            profits = [compute_profit(p, a, odd) for p, a, odd in zip(bets, actuals, odds_used)]\n",
    "            total_profit = sum(profits)\n",
    "            total_risk = sum(abs(odd) if p != a else 100 for p, a, odd in zip(bets, actuals, odds_used))  # optional\n",
    "            roi = (total_profit / total_risk) * 100 if total_risk > 0 else 0\n",
    "            accuracy = sum([p == a for p, a in zip(bets, actuals)]) / len(bets)\n",
    "\n",
    "            print(f\"    {over_thresh:.2f}     |     {under_thresh:.2f}     |    {len(bets):4}     |  {accuracy:.4f} |   {total_profit:6.2f}  |  {roi:6.2f}\")\n",
    "            category_results.append({\n",
    "                'category': category,\n",
    "                'model_name': model_name,\n",
    "                'over_thresh': over_thresh,\n",
    "                'under_thresh': under_thresh,\n",
    "                'bets': len(bets),\n",
    "                'accuracy': accuracy,\n",
    "                'total_profit': total_profit,\n",
    "                'roi': roi,\n",
    "                'calibrated_model': calibrated_model,  # Save model object itself\n",
    "                'features': list(x_train.columns),\n",
    "            })\n",
    "\n",
    "            results[model_name][category] = category_results\n",
    "\n",
    "\n",
    "\n",
    "# Passthrough transformer that keeps DataFrame structure\n",
    "# Identity transformer to preserve DataFrame structure in pipeline\n",
    "preserve_df = FunctionTransformer(lambda x: x)\n",
    "\n",
    "# Best-performing models wrapped in pipelines\n",
    "# best_models = {\n",
    "#     'pts': make_pipeline(preserve_df, RandomForestClassifier(n_estimators=150, max_depth=8, random_state=42)),\n",
    "#     'ast': make_pipeline(preserve_df, LogisticRegression(max_iter=1000)),\n",
    "#     'reb': make_pipeline(preserve_df, LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42)),\n",
    "#     '3pm': make_pipeline(preserve_df, RandomForestClassifier(n_estimators=150, max_depth=8, random_state=42)),\n",
    "# }\n",
    "\n",
    "# # Thresholds based on tuning\n",
    "# category_thresholds = {\n",
    "#     'pts': {'threshold_over': 0.65, 'threshold_under': 0.35},\n",
    "#     'ast': {'threshold_over': 0.60, 'threshold_under': 0.40},\n",
    "#     'reb': {'threshold_over': 0.75, 'threshold_under': 0.25},\n",
    "#     '3pm': {'threshold_over': 0.65, 'threshold_under': 0.35},\n",
    "# }\n",
    "\n",
    "# saved_models = {}\n",
    "\n",
    "# # Train and save each model\n",
    "# for category, model in best_models.items():\n",
    "#     df = full_data[category].copy()\n",
    "#     df = df.dropna(subset=['result'])\n",
    "#     df['result'] = df['result'].map(label_map) if df['result'].dtype == object else df['result']\n",
    "\n",
    "#     x = df[classification_features[category]].fillna(0)\n",
    "#     y = df['result']\n",
    "\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     model.fit(x_train, y_train)\n",
    "\n",
    "#     calibrated_model = CalibratedClassifierCV(model, cv='prefit')\n",
    "#     calibrated_model.fit(x_train, y_train)\n",
    "\n",
    "#     # Store calibrated model + the exact features used\n",
    "#     saved_models[category] = {\n",
    "#         'model': calibrated_model,\n",
    "#         'threshold_over': category_thresholds[category]['threshold_over'],\n",
    "#         'threshold_under': category_thresholds[category]['threshold_under'],\n",
    "#         'features': list(x_train.columns)\n",
    "#     }\n",
    "saved_models = {}\n",
    "\n",
    "MIN_BETS = 30  # Your cutoff for meaningful sample size\n",
    "\n",
    "for category in ['pts', 'ast', 'reb', '3pm']:\n",
    "    all_results = []\n",
    "    for model_name in results:\n",
    "        if category in results[model_name]:\n",
    "            all_results.extend(results[model_name][category])\n",
    "\n",
    "    # Filter by minimum bet count\n",
    "    filtered = [r for r in all_results if r['bets'] >= MIN_BETS]\n",
    "\n",
    "    if not filtered:\n",
    "        print(f\" No valid models found for {category} with enough bets.\")\n",
    "        continue\n",
    "\n",
    "    # Pick model with best ROI\n",
    "    best = max(filtered, key=lambda x: x['roi'])\n",
    "\n",
    "    saved_models[category] = {\n",
    "        'model': best['calibrated_model'],\n",
    "        'threshold_over': best['over_thresh'],\n",
    "        'threshold_under': best['under_thresh'],\n",
    "        'features': best['features'],\n",
    "        'model_name': best['model_name'],  # optional for tracking\n",
    "        'roi': best['roi'],  # optional\n",
    "        'bets': best['bets'],  # optional\n",
    "    }\n",
    "\n",
    "    print(f\"Best model for {category.upper()}: {best['model_name']} | ROI: {best['roi']:.2f}% | Bets: {best['bets']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Latest date used in regression training (data_ordered): 2023-04-09\n",
      "\n",
      "🚨 Checking for potential overlap between regression training and classification test sets:\n",
      "\n",
      "✅ PTS: Clean split — Regression ends 2023-04-09, classification test starts 2025-03-03\n",
      "✅ AST: Clean split — Regression ends 2023-04-09, classification test starts 2025-03-03\n",
      "✅ REB: Clean split — Regression ends 2023-04-09, classification test starts 2025-03-03\n",
      "✅ 3PM: Clean split — Regression ends 2023-04-09, classification test starts 2025-03-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68938/293419545.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['game_date'] = pd.to_datetime(train_data['game_date'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Categories\n",
    "categories = ['pts', 'ast', 'reb', '3pm']\n",
    "\n",
    "# --- STEP 1: Get Latest Regression Date from `data_ordered`\n",
    "train_data['game_date'] = pd.to_datetime(train_data['game_date'])\n",
    "regression_cutoff_date = train_data['game_date'].max()\n",
    "\n",
    "print(f\"Latest date used in regression training (data_ordered): {regression_cutoff_date.date()}\")\n",
    "\n",
    "# --- STEP 2: Get Earliest Test Date per Category\n",
    "test_start_dates = {}\n",
    "\n",
    "for category in categories:\n",
    "    df = full_data[category].copy()\n",
    "    df = df.dropna(subset=['result'])\n",
    "    df = df.sort_values('game_date')  # Just in case\n",
    "\n",
    "    x = df[classification_features[category]].fillna(0)\n",
    "    y = df['result']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    test_dates = df.loc[x_test.index, 'game_date']\n",
    "    test_start_dates[category] = test_dates.min()\n",
    "\n",
    "# --- STEP 3: Check for Overlap\n",
    "print(\"\\nChecking for potential overlap between regression training and classification test sets:\\n\")\n",
    "for cat in categories:\n",
    "    test_date = test_start_dates[cat]\n",
    "    if regression_cutoff_date >= test_date:\n",
    "        print(f\" {cat.upper()}: OVERLAP detected — Regression trained up to {regression_cutoff_date.date()}, classification test starts {test_date.date()}\")\n",
    "    else:\n",
    "        print(f\"{cat.upper()}: Clean split — Regression ends {regression_cutoff_date.date()}, classification test starts {test_date.date()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
