{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.linear_model._logistic import LogisticRegression\n",
    "from datetime import datetime as dt\n",
    "from sklearn.model_selection import TimeSeriesSplit \n",
    "from sklearn.metrics import r2_score,make_scorer,mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import os\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_player_name(name):\n",
    "    \"\"\"Standardizes player names by removing special characters and handling known name variations.\"\"\"\n",
    "    name = name.lower().strip()  # Convert to lowercase & remove extra spaces\n",
    "    name = name.replace(\".\", \"\")  # Remove periods\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove special characters (apostrophes, dashes, etc.) \n",
    "    # Known name changes (add more as needed)\n",
    "    name_corrections = {\n",
    "        \"alexandre sarr\": \"alex sarr\",\n",
    "        \"jimmy butler\": \"jimmy butler iii\",\n",
    "        \"nicolas claxton\": \"nic claxton\",\n",
    "        \"kenyon martin jr\": \"kj martin\",\n",
    "        \"carlton carrington\": \"bub carrington\",\n",
    "        \"ron holland ii\": \"ronald holland ii\",\n",
    "        'cameron thomas':'cam thomas'\n",
    "    }\n",
    "\n",
    "    # Apply corrections if the name exists in the dictionary\n",
    "    return name_corrections.get(name, name)  # Default to original name if no correction found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#using shifted windows for rolling data to prevent data leakage\n",
    "player_query = f\"\"\" \n",
    "SELECT *\n",
    "from `capstone_data.player_modeling_data_partitioned`\n",
    "order by game_date asc\n",
    "\"\"\"\n",
    "\n",
    "team_query = f\"\"\"\n",
    "SELECT *\n",
    "from `capstone_data.team_modeling_data_partitioned`\n",
    "order by game_date asc\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "try:\n",
    "    full_data = pd.read_csv('full_data.csv')\n",
    "\n",
    "except:\n",
    "    nba_player_data = pd.DataFrame(pandas_gbq.read_gbq(player_query,project_id='miscellaneous-projects-444203',progress_bar_type='tqdm'))\n",
    "    team_data = pd.DataFrame(pandas_gbq.read_gbq(team_query,project_id='miscellaneous-projects-444203',progress_bar_type='tqdm'))\n",
    "    team_data  = team_data.merge(team_data,on='game_id',suffixes=('',\"_opponent\"))\n",
    "    team_data = team_data[team_data[\"team_id\"] != team_data[\"team_id_opponent\"]]\n",
    "    full_data = nba_player_data.merge(team_data, on = ['game_id','team'], how = 'inner',suffixes=('','remove'))\n",
    "    full_data.drop([column for column in full_data.columns if 'remove' in column],axis = 1 , inplace=True) \n",
    "    full_data.drop([column for column in full_data.columns if '_1' in column],axis = 1 , inplace=True)\n",
    "    full_data.to_csv('full_data.csv',mode = 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered = full_data.sort_values('game_date')\n",
    "\n",
    "data_ordered.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Ideas \n",
    "\n",
    "* (ratio of 3pa and fga and 3pm and 3pa) TS% for players efg% \n",
    "* for players assist_to_turnover ratio assist ratio, \n",
    "* rebound_cahnce, defesnive reb %, \n",
    "* ast_ratio_season * pace, \n",
    "* home * pts season - data pts 3pm avg,\n",
    "* cold_streak pts_3gm_avg < pts_season boolean, \n",
    "* away difficulty away * opponent_defrtg_3gm_avg,\n",
    "* home_performance = data_ordered[data_ordered[\"home\"] == 1].groupby(\"team\")[\"pts_season\"].mean()\n",
    "* away_performance = data_ordered[data_ordered[\"away\"] == 1].groupby(\"team\")[\"pts_season\"].mean() these would be to see how the team performance changes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ordered['pts_per_min_3gm'] = data_ordered['pts_3gm_avg']/data_ordered['min_3gm_avg']\n",
    "# data_ordered['pts_per_min_season'] = data_ordered['pts_season']/data_ordered['min_season']\n",
    "# data_ordered['pts_per_min_momentum'] = data_ordered['pts_per_min_3gm'] - data_ordered['pts_per_min_season']\n",
    "\n",
    "# data_ordered['fg3m_per_min_3gm'] = data_ordered['fg3m_3gm_avg']/data_ordered['min_3gm_avg']\n",
    "# data_ordered['fg3m_per_min_season'] = data_ordered['fg3m_season']/data_ordered['min_season']\n",
    "# data_ordered['fg3m_per_min_momentum'] = data_ordered['fg3m_per_min_3gm'] - data_ordered['fg3m_per_min_season'] \n",
    "\n",
    "# data_ordered['reb_per_min_3gm'] = data_ordered['reb_3gm_avg']/data_ordered['min_3gm_avg']\n",
    "# data_ordered['reb_per_min_season'] = data_ordered['reb_season']/data_ordered['min_season']\n",
    "# data_ordered['reb_per_min_momentum'] = data_ordered['fg3m_per_min_3gm'] - data_ordered['reb_per_min_season']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered = data_ordered.groupby(['player','season']).apply(lambda x: x.iloc[3:]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered.sort_values(by='game_date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['game_date'] = pd.to_datetime(data_ordered['game_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['days_ago'] = (data_ordered['game_date'].max() - data_ordered['game_date']).dt.days\n",
    "data_ordered['time_decay_weight'] = 1 / (1 + np.log(1 + data_ordered['days_ago']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_ordered = data_ordered.drop('Unnamed: 0', axis =1)\n",
    "except KeyError:\n",
    "    print('Irregular column not made')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs with the column mean, but only for numeric columns\n",
    "data_ordered.fillna(data_ordered.select_dtypes(include=['number']).mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = data_ordered.select_dtypes(include=['number']).columns.tolist()\n",
    "numeric_columns = [column for column in numeric_columns if column not in ['pts','reb','ast','blk','stl','3pm','game_id','game_date','days_ago','time_decay_weight','team_id', \"gp_rank\", \"w_rank\", \"l_rank\", \"w_pct_rank\", \"min_rank\", \"fgm_rank\",\n",
    "    \"fga_rank\", \"fg_pct_rank\", \"fg3m_rank\", \"fg3a_rank\", \"fg3_pct_rank\",\n",
    "    \"ftm_rank\", \"fta_rank\", \"ft_pct_rank\", \"oreb_rank\", \"dreb_rank\",\n",
    "    \"reb_rank\", \"ast_rank\", \"tov_rank\", \"stl_rank\", \"blk_rank\",\n",
    "    \"blka_rank\", \"pf_rank\", \"pfd_rank\", \"pts_rank\", \"plus_minus_rank\",]]\n",
    "\n",
    "numeric_columns = [feature for feature in numeric_columns if any(keyword in feature for keyword in [\"3gm_avg\", \"season\", \"momentum\"])]\n",
    "features = {feature:[] for feature in ['pts','reb','ast','3pm']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for category in features.keys():\n",
    "    x = train_data[numeric_columns]\n",
    "    y = train_data[category]\n",
    "\n",
    "    mi_scores = mutual_info_regression(x, y)\n",
    "    mi_scores = pd.Series(mi_scores, index=numeric_columns)\n",
    "    selected_features = mi_scores[mi_scores > 0.10].index.tolist()  \n",
    "\n",
    "    features[category] = selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These values appeared to have non-linear relationships applying transformations\n",
    "# data_ordered['ft%_season'] = np.log1p(data_ordered['ft%_season'])\n",
    "# data_ordered['stl_3gm_avg'] = np.log1p(data_ordered['stl_3gm_avg'])\n",
    "# data_ordered['stl_season'] = np.log1p(data_ordered['stl_season'])\n",
    "# data_ordered['to_season'] = data_ordered['to_season']**2 \n",
    "# data_ordered['to_3gm_avg'] = data_ordered['to_3gm_avg']**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models = {category:{} for category in ['pts','reb','ast','3pm']} \n",
    "saved_results = {category:{} for category in ['pts','reb','ast','3pm']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP\n",
    "Applying shap to help reduce collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for category in features.keys():\n",
    "\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    print(len(features_list))\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    linear_model = LinearRegression()\n",
    "\n",
    "    linear_model.fit(x_train,y_train)\n",
    "\n",
    "    y_pred = linear_model.predict(x_test)\n",
    "    print(category)\n",
    "    print(r2_score(y_true=y_test,y_pred=y_pred))\n",
    "\n",
    "    saved_results[category]['linear_model']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n",
    "    saved_models[category]['linear_model'] = linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in features.keys():\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    ridge_model = Ridge(alpha=1)\n",
    "\n",
    "    ridge_model.fit(x_train,y_train)\n",
    "\n",
    "    output = pd.DataFrame({'prediction':ridge_model.predict(x_test), 'actual':y_test})\n",
    "    print(category)\n",
    "    print(r2_score(y_true=output['actual'],y_pred=output['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# features = {}\n",
    "\n",
    "# for category in ['pts', 'reb', 'ast', '3pm']:\n",
    "#     x_train = train_data[numeric_columns]\n",
    "#     y_train = train_data[category]\n",
    "\n",
    "#     # Use Lasso to select features automatically\n",
    "#     lasso = Lasso(alpha=0.01)  # Adjust alpha based on tuning\n",
    "#     lasso.fit(x_train, y_train)\n",
    "\n",
    "#     # Keep only features with nonzero coefficients\n",
    "#     selected_features = [f for f, coef in zip(numeric_columns, lasso.coef_) if coef != 0]\n",
    "\n",
    "#     # Cap the number of features (e.g., max 50)\n",
    "#     selected_features = selected_features[:min(len(selected_features), 50)]\n",
    "\n",
    "#     features[category] = selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for category in features.keys():\n",
    "    features_list = features[category]\n",
    "\n",
    "    x_train, y_train = train_data[features_list], train_data[category]\n",
    "    x_test, y_test = test_data[features_list], test_data[category]\n",
    "\n",
    "    linear_model = Ridge(alpha=1.0)  # Use Ridge instead of LinearRegression\n",
    "    linear_model.fit(x_train, y_train)\n",
    "\n",
    "    # Cross-validation score instead of just test R²\n",
    "    cv_r2 = cross_val_score(linear_model, x_train, y_train, cv=5, scoring='r2').mean()\n",
    "\n",
    "    y_pred = linear_model.predict(x_test)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{category}: Cross-Val R² = {cv_r2:.4f}, Test R² = {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(data_ordered[numeric_columns])\n",
    "\n",
    "scaled_data_df = pd.DataFrame(scaled_data,columns=numeric_columns)\n",
    "\n",
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "scaled_train_data = scaled_data_df.iloc[:split_index]\n",
    "scaled_test_data = scaled_data_df[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth':[3,6,9],'learning_rate':[.01,.05,.1,.3],'booster':['gbtree','dart'],'subsample':[.5,.7,.9],'colsample_bytree':[.5,.7,.9],'n_estimators':[100,300,500]}\n",
    "param_linear = {'booster':['gblinear'],'lambda':[0,.1,1,10],'alpha':[0,.1,1,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor = xgboost.XGBRegressor()\n",
    "mse_score = make_scorer(mean_squared_error,greater_is_better=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "scoring = {'MSE':mse_score,'r2':r2_scorer}\n",
    "grid_search = GridSearchCV(estimator=xgb_regressor,param_grid=param_grid,scoring = scoring,cv=tscv,n_jobs=1,verbose=0,refit='r2')\n",
    "grid_linear_search = GridSearchCV(estimator=xgb_regressor,param_grid=param_linear,scoring = scoring,cv=tscv,n_jobs=3,verbose=0,refit='r2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_features =  features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in features.keys():\n",
    "    x_train,y_train = scaled_train_data[xg_features[category]],train_data[category]\n",
    "    x_test, y_test = scaled_test_data[xg_features[category]],test_data[category]\n",
    "\n",
    "    fit_params = {'eval_set':[(x_test,y_test)],'early_stopping_rounds':20,'verbose':False}\n",
    "\n",
    "    grid_linear_search.estimator.set_params(eval_metric='rmse')\n",
    "\n",
    "\n",
    "    grid_linear_search.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "    print(category)\n",
    "    print(grid_linear_search.best_params_)\n",
    "    print(grid_linear_search.best_score_)\n",
    "\n",
    "    y_pred = grid_linear_search.best_estimator_.predict(x_test)\n",
    "\n",
    "    saved_models[category]['XGboost'] = grid_linear_search.best_estimator_\n",
    "    saved_results[category]['XGboost']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "lgb_model = LGBMRegressor(n_estimators=1000, random_state=42,verbosity=-1)\n",
    "\n",
    "# Define the expanded parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [15, 31, 50, 75],\n",
    "    'learning_rate': [0.005, 0.01, 0.05],\n",
    "    'max_depth': [-1, 5, 10, 15],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Time series split (if your data is chronological)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Randomized search setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=40,  # control number of total combinations to test\n",
    "    cv=tscv,\n",
    "    scoring='r2',\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit to your training data\n",
    "# Best model + params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]\n",
    "for category in features.keys():\n",
    "    x_train,y_train = train_data[features[category]],train_data[category]\n",
    "    x_test,y_test = test_data[features[category]],test_data[category]\n",
    "\n",
    "    random_search.fit(x_train,y_train)\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    print(category)\n",
    "    print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "\n",
    "    saved_models[category]['lightgbm'] = best_model\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'R2: {r2}')\n",
    "\n",
    "    saved_results[category]['lightgbm']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(saved_models,'models.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_performance.txt', 'w') as file:\n",
    "    for category, models in saved_results.items():\n",
    "        file.write(f\"Category: {category}\\n\")\n",
    "        for model, metrics in models.items():\n",
    "            file.write(f\"  Model: {model}\\n\")\n",
    "            for metric, value in metrics.items():\n",
    "                file.write(f\"    {metric}: {value}\\n\")\n",
    "        file.write(\"\\n\")  # Newline between categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Modeling into Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble modeling\n",
    "\n",
    "saved_models = joblib.load('models.pkl')\n",
    "\n",
    "linear_models = {cat: saved_models[cat]['linear_model'] for cat in models if 'linear_model' in saved_models[cat]}\n",
    "lightgbm_models = {cat: saved_models[cat]['lightgbm'] for cat in models if 'lightgbm' in saved_models[cat]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "meta_models = {}\n",
    "meta_results = {}\n",
    "\n",
    "for category in ['pts', 'reb', 'ast', '3pm']:\n",
    "    # Load models\n",
    "    lm = saved_models[category]['linear_model']\n",
    "    lgb = saved_models[category]['lightgbm']\n",
    "\n",
    "    # Prepare test data\n",
    "    lm_features_list = [f.strip() for f in lm.feature_names_in_]\n",
    "    lgb_features_list = [f.strip() for f in lgb.feature_names_in_]\n",
    "\n",
    "    lm_x_test = test_data[lm_features_list]\n",
    "\n",
    "    lgb_x_test = test_data[lgb_features_list]\n",
    "\n",
    "    y_test = test_data[category]\n",
    "\n",
    "    # Get predictions\n",
    "    preds_lm = lm.predict(lm_x_test)\n",
    "    preds_lgb = lgb.predict(lgb_x_test)\n",
    "\n",
    "    # Stack predictions into meta-model features\n",
    "    meta_X = np.vstack([preds_lm, preds_lgb]).T\n",
    "    meta_y = y_test.values\n",
    "\n",
    "    # Train meta-model\n",
    "    meta_model = Ridge()\n",
    "    meta_model.fit(meta_X, meta_y)\n",
    "\n",
    "    # Evaluate meta-model\n",
    "    meta_preds = meta_model.predict(meta_X)\n",
    "    r2 = r2_score(meta_y, meta_preds)\n",
    "    mse = mean_squared_error(meta_y, meta_preds)\n",
    "\n",
    "    print(f\"{category} Meta-model R²: {r2:.4f}, MSE: {mse:.4f}\")\n",
    "\n",
    "    meta_models[category] = meta_model\n",
    "    meta_results[category] = {'r2': r2, 'mse': mse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(meta_models,'meta_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = {}\n",
    "for category, model in meta_models.items():\n",
    "    coef_linear, coef_lgbm = model.coef_\n",
    "    print(f\"{category.upper()} Meta-Model Weights:\")\n",
    "    print(f\"  Linear Model Weight:  {coef_linear:.4f}\")\n",
    "    print(f\"  LightGBM Weight:      {coef_lgbm:.4f}\")\n",
    "\n",
    "    coef[f'{category}_lm'] = coef_linear\n",
    "    coef[f'{category}_lgb'] = coef_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Model into Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['points','assists','rebounds','threes_made']\n",
    "categories = ['pts','ast','reb','3pm']\n",
    "odds_data = {}\n",
    "for cat,category in zip(cats,categories):\n",
    "    predictions_query = f\"\"\"\n",
    "    select \n",
    "        po.*,\n",
    "        pp.{category}_linear_model,\n",
    "        pp.{category}_lightgbm\n",
    "    from `capstone_data.{cat}_predictions` pp\n",
    "    inner join `capstone_data.{category}_outcome` po\n",
    "        on pp.Player = po.player and date(pp.Date_Updated) = po.game_date\n",
    "    where pp.{category}_linear_model is not null\n",
    "    \"\"\" \n",
    "    data = pandas_gbq.read_gbq(predictions_query,project_id='miscellaneous-projects-444203')\n",
    "\n",
    "    \n",
    "    odds_data[category] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = {category:[] for category in categories}\n",
    "\n",
    "data_ordered['player'] = data_ordered['player'].apply(clean_player_name)\n",
    "\n",
    "for category in categories:\n",
    "\n",
    "    data = odds_data[category]\n",
    "    \n",
    "    data['game_date'] = pd.to_datetime(data['game_date'])\n",
    "\n",
    "    data['player']= data['player'].apply(clean_player_name)\n",
    "\n",
    "    data = data.merge(data_ordered, on=['player','game_date'],how='inner')\n",
    "\n",
    "    data = data.drop_duplicates(subset=['player', 'game_date'], keep='first')\n",
    "\n",
    "    full_data[category] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat,category in zip(cats,categories):\n",
    "    full_data[category][f'{category}_ensemble'] = (pd.to_numeric(full_data[category][f'{category}_linear_model']) * \n",
    "    coef[f'{category}_lm'] + pd.to_numeric(full_data[category][f'{category}_lightgbm'] * coef[f'{category}_lgb']))\n",
    "\n",
    "    full_data[category][f'{category}_delta'] = full_data[category][f'{cat}'] - full_data[category][f'{category}_ensemble']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_features = {}\n",
    "\n",
    "for cat,category in zip(cats,categories):\n",
    "    \n",
    "    lm = saved_models[category]['linear_model']\n",
    "    lgb = saved_models[category]['lightgbm']\n",
    "\n",
    "    lm_features_list = [f.strip() for f in lm.feature_names_in_]\n",
    "    lgb_features_list = [f.strip() for f in lgb.feature_names_in_]\n",
    "\n",
    "    full_features = list(set(lm_features_list + lgb_features_list))\n",
    "    \n",
    "    full_features.append(f'{category}_delta')\n",
    "    full_features.append(f'{category}_ensemble')\n",
    "    full_features.append(f'{cat}')\n",
    "\n",
    "    corr_matrix = full_data[category][full_features].corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    classification_features[category] = full_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    print(category)\n",
    "    print(classification_features[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for category in categories:\n",
    "    split_index = int(len(full_data[category]) * 0.80)\n",
    "\n",
    "    train_data = full_data[category].iloc[:split_index]\n",
    "    test_data = full_data[category].iloc[split_index:]\n",
    "\n",
    "    X_train = train_data[classification_features[category]]\n",
    "    y_train = train_data['result']\n",
    "    X_test = test_data[classification_features[category]]\n",
    "    y_test = test_data['result']\n",
    "\n",
    "    logistic = LogisticRegression(max_iter=1000)\n",
    "    logistic.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = logistic.predict(X_test)\n",
    "    y_prob = logistic.predict_proba(X_test)[:, 1]  # probs for positive class\n",
    "\n",
    "    print(f\"\\n=== Category: {category} ===\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Per-category thresholds (adjustable)\n",
    "thresholds = {\n",
    "    'pts': (0.65, 0.35),\n",
    "    'ast': (0.68, 0.32),\n",
    "    'reb': (0.66, 0.34),\n",
    "    '3pm': (0.70, 0.30)\n",
    "}\n",
    "\n",
    "# Label mapping\n",
    "label_map = {'Over': 1, 'Under': 0}\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\n=== Category: {category.upper()} ===\")\n",
    "\n",
    "    over_thresh, under_thresh = thresholds[category]\n",
    "\n",
    "    # Train/test split\n",
    "    split_index = int(len(full_data[category]) * 0.80)\n",
    "    train_data = full_data[category].iloc[:split_index]\n",
    "    test_data = full_data[category].iloc[split_index:]\n",
    "\n",
    "    X_train_full = train_data[classification_features[category]]\n",
    "    y_train = train_data['result'].map(label_map) if isinstance(train_data['result'].iloc[0], str) else train_data['result']\n",
    "    X_test_full = test_data[classification_features[category]]\n",
    "    y_test = test_data['result'].map(label_map) if isinstance(test_data['result'].iloc[0], str) else test_data['result']\n",
    "\n",
    "    # Scale all features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled_full = scaler.fit_transform(X_train_full)\n",
    "    X_test_scaled_full = scaler.transform(X_test_full)\n",
    "\n",
    "    # Initial RF model for feature importance\n",
    "    rf_temp = RandomForestClassifier(n_estimators=200, max_depth=8, min_samples_split=5,\n",
    "                                     min_samples_leaf=3, class_weight='balanced_subsample', random_state=42)\n",
    "    rf_temp.fit(X_train_scaled_full, y_train)\n",
    "\n",
    "    # Select top 15 features\n",
    "    importances = rf_temp.feature_importances_\n",
    "    feature_names = X_train_full.columns\n",
    "    top_features_idx = np.argsort(importances)[::-1][:15]\n",
    "    top_features = feature_names[top_features_idx]\n",
    "\n",
    "    X_train = X_train_full[top_features]\n",
    "    X_test = X_test_full[top_features]\n",
    "\n",
    "    # Scale reduced set\n",
    "    scaler_reduced = StandardScaler()\n",
    "    X_train_scaled = scaler_reduced.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_reduced.transform(X_test)\n",
    "\n",
    "    # Final tuned and calibrated Random Forest\n",
    "    rf_model = RandomForestClassifier(n_estimators=200, max_depth=8, min_samples_split=5,\n",
    "                                      min_samples_leaf=3, class_weight='balanced_subsample', random_state=42)\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    calibrated_model = CalibratedClassifierCV(rf_model, cv='prefit')\n",
    "    calibrated_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict calibrated probabilities\n",
    "    y_prob = calibrated_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # Generate recommendations\n",
    "    recommendations = []\n",
    "    for prob in y_prob:\n",
    "        if prob >= over_thresh:\n",
    "            recommendations.append(1)\n",
    "        elif prob <= under_thresh:\n",
    "            recommendations.append(0)\n",
    "        else:\n",
    "            recommendations.append(None)\n",
    "\n",
    "    # Evaluate filtered predictions\n",
    "    filtered_preds = [int(pred) for pred in recommendations if pred is not None]\n",
    "    filtered_actuals = [int(actual) for pred, actual in zip(recommendations, y_test) if pred is not None]\n",
    "\n",
    "    print(f\"Total bets recommended: {len(filtered_preds)} out of {len(y_test)}\")\n",
    "    if filtered_preds:\n",
    "        print(f\"Accuracy on recommended bets: {accuracy_score(filtered_actuals, filtered_preds):.4f}\")\n",
    "        print(\"Classification Report (Filtered):\")\n",
    "        print(classification_report(filtered_actuals, filtered_preds))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(filtered_actuals, filtered_preds))\n",
    "    else:\n",
    "        print(\"⚠️ No bets met the confidence threshold.\")\n",
    "\n",
    "    # Final feature importance on reduced set\n",
    "    final_importance = rf_model.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': top_features,\n",
    "        'Importance': final_importance\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    print(f\"\\nTop Features for {category.upper()}:\")\n",
    "    print(importance_df.to_string(index=False))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
