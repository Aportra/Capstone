{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.linear_model._logistic import LogisticRegression\n",
    "from datetime import datetime as dt\n",
    "from sklearn.model_selection import TimeSeriesSplit \n",
    "from sklearn.metrics import r2_score,make_scorer,mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import os\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_player_name(name):\n",
    "    \"\"\"Standardizes player names by removing special characters and handling known name variations.\"\"\"\n",
    "    name = name.lower().strip()  # Convert to lowercase & remove extra spaces\n",
    "    name = name.replace(\".\", \"\")  # Remove periods\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove special characters (apostrophes, dashes, etc.) \n",
    "    # Known name changes (add more as needed)\n",
    "    name_corrections = {\n",
    "        \"alexandre sarr\": \"alex sarr\",\n",
    "        \"jimmy butler\": \"jimmy butler iii\",\n",
    "        \"nicolas claxton\": \"nic claxton\",\n",
    "        \"kenyon martin jr\": \"kj martin\",\n",
    "        \"carlton carrington\": \"bub carrington\",\n",
    "        \"ron holland ii\": \"ronald holland ii\",\n",
    "        'cameron thomas':'cam thomas'\n",
    "    }\n",
    "\n",
    "    # Apply corrections if the name exists in the dictionary\n",
    "    return name_corrections.get(name, name)  # Default to original name if no correction found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#using shifted windows for rolling data to prevent data leakage\n",
    "player_query = f\"\"\" \n",
    "SELECT *\n",
    "from `capstone_data.player_modeling_data_partitioned`\n",
    "order by game_date asc\n",
    "\"\"\"\n",
    "\n",
    "team_query = f\"\"\"\n",
    "SELECT *\n",
    "from `capstone_data.team_modeling_data_partitioned`\n",
    "order by game_date asc\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        \"/home/aportra99/scraping_key.json\"\n",
    "    )\n",
    "    local = False\n",
    "    print(\"Credentials file loaded.\")\n",
    "except FileNotFoundError:\n",
    "    local = True\n",
    "    credentials = None\n",
    "    print(\"Running with default credentials.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_player_data = pd.DataFrame(pandas_gbq.read_gbq(player_query,project_id='miscellaneous-projects-444203',progress_bar_type='tqdm',credentials=credentials))\n",
    "team_data = pd.DataFrame(pandas_gbq.read_gbq(team_query,project_id='miscellaneous-projects-444203',progress_bar_type='tqdm',credentials=credentials))\n",
    "team_data  = team_data.merge(team_data,on='game_id',suffixes=('',\"_opponent\"))\n",
    "team_data = team_data[team_data[\"team_id\"] != team_data[\"team_id_opponent\"]]\n",
    "full_data = nba_player_data.merge(team_data, on = ['game_id','team'], how = 'inner',suffixes=('','remove'))\n",
    "full_data.drop([column for column in full_data.columns if 'remove' in column],axis = 1 , inplace=True) \n",
    "full_data.drop([column for column in full_data.columns if '_1' in column],axis = 1 , inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered = full_data.sort_values('game_date')\n",
    "\n",
    "data_ordered.dropna(inplace=True,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Ideas \n",
    "\n",
    "* (ratio of 3pa and fga and 3pm and 3pa) TS% for players efg% \n",
    "* for players assist_to_turnover ratio assist ratio, \n",
    "* rebound_cahnce, defesnive reb %, \n",
    "* ast_ratio_season * pace, \n",
    "* home * pts season - data pts 3pm avg,\n",
    "* cold_streak pts_3gm_avg < pts_season boolean, \n",
    "* away difficulty away * opponent_defrtg_3gm_avg,\n",
    "* home_performance = data_ordered[data_ordered[\"home\"] == 1].groupby(\"team\")[\"pts_season\"].mean()\n",
    "* away_performance = data_ordered[data_ordered[\"away\"] == 1].groupby(\"team\")[\"pts_season\"].mean() these would be to see how the team performance changes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered = data_ordered.groupby(['player','season']).apply(lambda x: x.iloc[3:]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered.sort_values(by='game_date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['game_date'] = pd.to_datetime(data_ordered['game_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['days_ago'] = (data_ordered['game_date'].max() - data_ordered['game_date']).dt.days\n",
    "data_ordered['time_decay_weight'] = 1 / (1 + np.log(1 + data_ordered['days_ago']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_ordered = data_ordered.drop('Unnamed: 0', axis =1)\n",
    "except KeyError:\n",
    "    print('Irregular column not made')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs with the column mean, but only for numeric columns\n",
    "data_ordered.fillna(data_ordered.select_dtypes(include=['number']).mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = data_ordered.select_dtypes(include=['number']).columns.tolist()\n",
    "numeric_columns = [column for column in numeric_columns if column not in ['pts','reb','ast','blk','stl','3pm','game_id','game_date','days_ago','time_decay_weight','team_id', \"gp_rank\", \"w_rank\", \"l_rank\", \"w_pct_rank\", \"min_rank\", \"fgm_rank\",\n",
    "    \"fga_rank\", \"fg_pct_rank\", \"fg3m_rank\", \"fg3a_rank\", \"fg3_pct_rank\",\n",
    "    \"ftm_rank\", \"fta_rank\", \"ft_pct_rank\", \"oreb_rank\", \"dreb_rank\",\n",
    "    \"reb_rank\", \"ast_rank\", \"tov_rank\", \"stl_rank\", \"blk_rank\",\n",
    "    \"blka_rank\", \"pf_rank\", \"pfd_rank\", \"pts_rank\", \"plus_minus_rank\",]]\n",
    "\n",
    "numeric_columns = [feature for feature in numeric_columns if any(keyword in feature for keyword in [\"3gm_avg\", \"season\", \"momentum\"])]\n",
    "features = {feature:[] for feature in ['pts','reb','ast','3pm']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for category in features.keys():\n",
    "    x = train_data[numeric_columns]\n",
    "    y = train_data[category]\n",
    "\n",
    "    mi_scores = mutual_info_regression(x, y)\n",
    "    mi_scores = pd.Series(mi_scores, index=numeric_columns)\n",
    "    selected_features = mi_scores[mi_scores > 0.10].index.tolist()  \n",
    "\n",
    "    features[category] = selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models = {category:{} for category in ['pts','reb','ast','3pm']} \n",
    "saved_results = {category:{} for category in ['pts','reb','ast','3pm']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP\n",
    "Applying shap to help reduce collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for category in features.keys():\n",
    "\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    print(len(features_list))\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    linear_model = LinearRegression()\n",
    "\n",
    "    linear_model.fit(x_train,y_train)\n",
    "\n",
    "    y_pred = linear_model.predict(x_test)\n",
    "    print(category)\n",
    "    print(r2_score(y_true=y_test,y_pred=y_pred))\n",
    "\n",
    "    saved_results[category]['linear_model']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n",
    "    saved_models[category]['linear_model'] = linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in features.keys():\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    ridge_model = Ridge(alpha=1)\n",
    "\n",
    "    ridge_model.fit(x_train,y_train)\n",
    "\n",
    "    output = pd.DataFrame({'prediction':ridge_model.predict(x_test), 'actual':y_test})\n",
    "    print(category)\n",
    "    print(r2_score(y_true=output['actual'],y_pred=output['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for category in features.keys():\n",
    "    features_list = features[category]\n",
    "\n",
    "    x_train, y_train = train_data[features_list], train_data[category]\n",
    "    x_test, y_test = test_data[features_list], test_data[category]\n",
    "\n",
    "    linear_model = Ridge(alpha=1.0)  # Use Ridge instead of LinearRegression\n",
    "    linear_model.fit(x_train, y_train)\n",
    "\n",
    "    # Cross-validation score instead of just test R²\n",
    "    cv_r2 = cross_val_score(linear_model, x_train, y_train, cv=5, scoring='r2').mean()\n",
    "\n",
    "    y_pred = linear_model.predict(x_test)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{category}: Cross-Val R² = {cv_r2:.4f}, Test R² = {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "lgb_model = LGBMRegressor(n_estimators=1000, random_state=42,verbosity=-1)\n",
    "\n",
    "# Define the expanded parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [15, 31, 50, 75],\n",
    "    'learning_rate': [0.005, 0.01, 0.05],\n",
    "    'max_depth': [-1, 5, 10, 15],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Time series split (if your data is chronological)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Randomized search setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=40,  # control number of total combinations to test\n",
    "    cv=tscv,\n",
    "    scoring='r2',\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit to your training data\n",
    "# Best model + params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]\n",
    "for category in features.keys():\n",
    "    x_train,y_train = train_data[features[category]],train_data[category]\n",
    "    x_test,y_test = test_data[features[category]],test_data[category]\n",
    "\n",
    "    random_search.fit(x_train,y_train)\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    print(category)\n",
    "    print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "\n",
    "    saved_models[category]['lightgbm'] = best_model\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'R2: {r2}')\n",
    "\n",
    "    saved_results[category]['lightgbm']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(saved_models,'models.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_performance.txt', 'w') as file:\n",
    "    for category, models in saved_results.items():\n",
    "        file.write(f\"Category: {category}\\n\")\n",
    "        for model, metrics in models.items():\n",
    "            file.write(f\"  Model: {model}\\n\")\n",
    "            for metric, value in metrics.items():\n",
    "                file.write(f\"    {metric}: {value}\\n\")\n",
    "        file.write(\"\\n\")  # Newline between categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Modeling into Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble modeling\n",
    "\n",
    "saved_models = joblib.load('models.pkl')\n",
    "\n",
    "linear_models = {cat: saved_models[cat]['linear_model'] for cat in saved_models if 'linear_model' in saved_models[cat]}\n",
    "lightgbm_models = {cat: saved_models[cat]['lightgbm'] for cat in saved_models if 'lightgbm' in saved_models[cat]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "meta_models = {}\n",
    "meta_results = {}\n",
    "\n",
    "for category in ['pts', 'reb', 'ast', '3pm']:\n",
    "    # Load models\n",
    "    lm = saved_models[category]['linear_model']\n",
    "    lgb = saved_models[category]['lightgbm']\n",
    "\n",
    "    # Prepare test data\n",
    "    lm_features_list = [f.strip() for f in lm.feature_names_in_]\n",
    "    lgb_features_list = [f.strip() for f in lgb.feature_names_in_]\n",
    "\n",
    "    lm_x_test = test_data[lm_features_list]\n",
    "\n",
    "    lgb_x_test = test_data[lgb_features_list]\n",
    "\n",
    "    y_test = test_data[category]\n",
    "\n",
    "    # Get predictions\n",
    "    preds_lm = lm.predict(lm_x_test)\n",
    "    preds_lgb = lgb.predict(lgb_x_test)\n",
    "\n",
    "    # Stack predictions into meta-model features\n",
    "    meta_X = np.vstack([preds_lm, preds_lgb]).T\n",
    "    meta_y = y_test.values\n",
    "\n",
    "    # Train meta-model\n",
    "    meta_model = Ridge()\n",
    "    meta_model.fit(meta_X, meta_y)\n",
    "\n",
    "    # Evaluate meta-model\n",
    "    meta_preds = meta_model.predict(meta_X)\n",
    "    r2 = r2_score(meta_y, meta_preds)\n",
    "    mse = mean_squared_error(meta_y, meta_preds)\n",
    "\n",
    "    print(f\"{category} Meta-model R²: {r2:.4f}, MSE: {mse:.4f}\")\n",
    "\n",
    "    meta_models[category] = meta_model\n",
    "    meta_results[category] = {'r2': r2, 'mse': mse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(meta_models,'meta_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = {}\n",
    "for category, model in meta_models.items():\n",
    "    coef_linear, coef_lgbm = model.coef_\n",
    "    print(f\"{category.upper()} Meta-Model Weights:\")\n",
    "    print(f\"  Linear Model Weight:  {coef_linear:.4f}\")\n",
    "    print(f\"  LightGBM Weight:      {coef_lgbm:.4f}\")\n",
    "\n",
    "    coef[f'{category}_lm'] = coef_linear\n",
    "    coef[f'{category}_lgb'] = coef_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Model into Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_profit(pred, actual, odds):\n",
    "    if pred == actual:\n",
    "        return 100 if odds < 0 else odds\n",
    "    else:\n",
    "        return -abs(odds) if odds < 0 else -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['points','assists','rebounds','threes_made']\n",
    "categories = ['pts','ast','reb','3pm']\n",
    "odds_data = {}\n",
    "for cat,category in zip(cats,categories):\n",
    "    predictions_query = f\"\"\"\n",
    "    select *\n",
    "    from `capstone_data.{category}_outcome`\n",
    "    order by game_date asc\n",
    "    \"\"\" \n",
    "    data = pandas_gbq.read_gbq(predictions_query,project_id='miscellaneous-projects-444203',credentials=credentials)\n",
    "\n",
    "    \n",
    "    odds_data[category] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['game_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = {category:[] for category in categories}\n",
    "\n",
    "data_ordered['player'] = data_ordered['player'].apply(clean_player_name)\n",
    "\n",
    "for category in categories:\n",
    "\n",
    "    data = odds_data[category]\n",
    "    \n",
    "    data['game_date'] = pd.to_datetime(data['game_date'])\n",
    "\n",
    "    data['player']= data['player'].apply(clean_player_name)\n",
    "\n",
    "    data = data.merge(data_ordered, on=['player','game_date'],how='inner')\n",
    "\n",
    "    data = data.drop_duplicates(subset=['player', 'game_date'], keep='first')\n",
    "\n",
    "    full_data[category] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat,category in zip(cats,categories):\n",
    "    full_data[category][f'{category}_ensemble'] = (pd.to_numeric(full_data[category][f'{category}_linear_model']) * \n",
    "    coef[f'{category}_lm'] + pd.to_numeric(full_data[category][f'{category}_lightgbm'] * coef[f'{category}_lgb']))\n",
    "\n",
    "    full_data[category][f'{category}_delta'] = full_data[category][f'{cat}'] - full_data[category][f'{category}_ensemble']\n",
    "\n",
    "    full_data[category].sort_values(by='game_date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_features = {}\n",
    "mi_feature_sets = {}\n",
    "categories = ['pts', 'ast', 'reb', '3pm']\n",
    "\n",
    "box_score_cols = [\n",
    "    'pts', 'pts_y', 'pts_x', 'ast_x', 'ast_y', 'reb_x', 'reb_y', 'reb', 'ast',\n",
    "    'stl', 'blk', 'to', 'pf', 'min', 'fgm', 'fga', 'fg_pct',\n",
    "    '3pm_y', 'fg3a', 'fg3_pct', 'ftm', 'fta', 'ft_pct',\n",
    "    'oreb', 'dreb', 'plus_minus', '3pm', '3pm_x'\n",
    "]\n",
    "exclude_cols = box_score_cols + ['result', 'player', 'game_date', 'team', 'team_city', 'matchup', 'matchup_opponent']\n",
    "\n",
    "NUM_RUNS = 10\n",
    "SEEDS = list(range(42, 42 + NUM_RUNS))\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\n=== Processing category: {category.upper()} ===\")\n",
    "    data = full_data[category].copy()\n",
    "\n",
    "    if isinstance(data['result'].iloc[0], str):\n",
    "        data['result'] = data['result'].map({'Over': 1, 'Under': 0})\n",
    "\n",
    "    data = data.dropna(subset=['result'])\n",
    "\n",
    "    # Define features\n",
    "    candidate_features = [col for col in data.columns if col not in exclude_cols]\n",
    "    numeric_features = [col for col in candidate_features if pd.api.types.is_numeric_dtype(data[col])]\n",
    "\n",
    "    X = data[numeric_features].fillna(0)\n",
    "    y = data['result']\n",
    "\n",
    "    # ➤ Split first (no shuffle to preserve game ordering if time-based)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    # ➤ Standard scale only on training set\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # ➤ MI on training data only\n",
    "    mi_matrix = []\n",
    "    for seed in SEEDS:\n",
    "        mi_scores = mutual_info_classif(X_train_scaled, y_train, random_state=seed)\n",
    "        mi_matrix.append(mi_scores)\n",
    "\n",
    "    mi_avg_scores = np.mean(mi_matrix, axis=0)\n",
    "    mi_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Avg_MI_Score': mi_avg_scores\n",
    "    }).sort_values(by='Avg_MI_Score', ascending=False)\n",
    "\n",
    "    # ➤ Select top 15 features\n",
    "    top_features = mi_df['Feature'].head(15).tolist()\n",
    "    mi_feature_sets[category] = top_features\n",
    "    classification_features[category] = top_features\n",
    "\n",
    "    print(f\"Top averaged MI features for {category.upper()}:\")\n",
    "    print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage\n",
    "results = {}\n",
    "best_models = {}\n",
    "label_map = {'Over': 1, 'Under': 0}\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=150, max_depth=8, random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42),\n",
    "    'Stacked': StackingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=100, random_state=42)),\n",
    "        ],\n",
    "        final_estimator=LogisticRegression()\n",
    "    )\n",
    "}\n",
    "# Weighted scoring function\n",
    "def weighted_score(roi, profit, bets):\n",
    "    roi_score = roi\n",
    "    profit_score = profit / 1000\n",
    "    volume_score = np.log1p(bets)\n",
    "    return roi_score * .1 + profit_score * 0.50 + volume_score * 0.40\n",
    "\n",
    "# American odds payout\n",
    "def compute_profit(pred, actual, odds):\n",
    "    return 100 if pred == actual and odds < 0 else (odds if pred == actual else -abs(odds) if odds < 0 else -100)\n",
    "\n",
    "# Main loop\n",
    "for category in ['pts', 'ast', 'reb', '3pm']:\n",
    "    print(f\"\\n=== CATEGORY: {category.upper()} ===\")\n",
    "    best_entry = {'score': -float('inf')}  # ← init with low score\n",
    "\n",
    "    df = full_data[category].copy()\n",
    "    df = df.dropna(subset=['result'])\n",
    "    df['result'] = df['result'].map(label_map) if df['result'].dtype == object else df['result']\n",
    "\n",
    "    x = df[classification_features[category]].fillna(0)\n",
    "    y = df['result']\n",
    "    over_odds = df['Over'].values\n",
    "    under_odds = df['Under'].values\n",
    "\n",
    "    x_train, x_test, y_train, y_test, over_train, over_test, under_train, under_test = train_test_split(\n",
    "        x, y, over_odds, under_odds, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n--- Model: {model_name} ---\")\n",
    "        base_model = model\n",
    "        base_model.fit(x_train, y_train)\n",
    "        calibrated_model = CalibratedClassifierCV(base_model, cv='prefit')\n",
    "        calibrated_model.fit(x_train, y_train)\n",
    "        y_prob = calibrated_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "        thresholds = [(0.5 + i * 0.05, 0.5 - i * 0.05) for i in range(1, 7)]\n",
    "        print(\"Threshold_Over | Threshold_Under | Bets_Placed | Accuracy | Profit ($) | ROI (%)\")\n",
    "        print(\"-\" * 75)\n",
    "\n",
    "        for over_thresh, under_thresh in thresholds:\n",
    "            bets, actuals, odds_used = [], [], []\n",
    "\n",
    "            for prob, actual, o, u in zip(y_prob, y_test, over_test, under_test):\n",
    "                if prob >= over_thresh:\n",
    "                    bets.append(1)\n",
    "                    actuals.append(actual)\n",
    "                    odds_used.append(o)\n",
    "                elif prob <= under_thresh:\n",
    "                    bets.append(0)\n",
    "                    actuals.append(actual)\n",
    "                    odds_used.append(u)\n",
    "\n",
    "            if not bets or len(bets) < 25:\n",
    "                continue\n",
    "\n",
    "            profits = [compute_profit(p, a, odd) for p, a, odd in zip(bets, actuals, odds_used)]\n",
    "            total_profit = sum(profits)\n",
    "            total_risk = sum(abs(odd) if p != a else 100 for p, a, odd in zip(bets, actuals, odds_used))\n",
    "            roi = (total_profit / total_risk) * 100 if total_risk > 0 else 0\n",
    "            accuracy = sum([p == a for p, a in zip(bets, actuals)]) / len(bets)\n",
    "            score = weighted_score(roi, total_profit, len(bets))  # ✅ now safe to compute\n",
    "\n",
    "            print(f\"    {over_thresh:.2f}     |     {under_thresh:.2f}     |    {len(bets):4}     |  {accuracy:.4f} |   {total_profit:7.2f}  |   {roi:6.2f}\")\n",
    "\n",
    "            # Use weighted score to track best\n",
    "            if score > best_entry['score']:\n",
    "                best_entry = {\n",
    "                    'Model': model_name,\n",
    "                    'Category': category.upper(),\n",
    "                    'Over_Threshold': over_thresh,\n",
    "                    'Under_Threshold': under_thresh,\n",
    "                    'ROI': roi,\n",
    "                    'Profit': total_profit,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'Bets_Placed': len(bets),\n",
    "                    'Fitted_Model': calibrated_model,\n",
    "                    'score': score  # track score\n",
    "                }\n",
    "\n",
    "    best_models[category] = best_entry\n",
    "\n",
    "# Final summary\n",
    "summary_df = pd.DataFrame.from_dict(best_models, orient='index').reset_index(drop=True)\n",
    "summary_df = summary_df[['Category', 'Model', 'Over_Threshold', 'Under_Threshold', 'ROI', 'Profit', 'Accuracy', 'Bets_Placed']]\n",
    "summary_df = summary_df.sort_values(by='Category', ascending=True)\n",
    "\n",
    "print(\"\\n=== BEST MODELS BY CATEGORY ===\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save to .pkl\n",
    "with open('classification_models.pkl', 'wb') as f:\n",
    "    pickle.dump(best_models, f)\n",
    "\n",
    "print(\"\\nSaved best models and thresholds to 'best_models_and_thresholds.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Categories\n",
    "categories = ['pts', 'ast', 'reb', '3pm']\n",
    "\n",
    "# --- STEP 1: Get Latest Regression Date from `data_ordered`\n",
    "train_data['game_date'] = pd.to_datetime(train_data['game_date'])\n",
    "regression_cutoff_date = train_data['game_date'].max()\n",
    "\n",
    "print(f\"Latest date used in regression training (data_ordered): {regression_cutoff_date.date()}\")\n",
    "\n",
    "# --- STEP 2: Get Earliest Test Date per Category\n",
    "test_start_dates = {}\n",
    "\n",
    "for category in categories:\n",
    "    df = full_data[category].copy()\n",
    "    df = df.dropna(subset=['result'])\n",
    "    df = df.sort_values('game_date')  # Just in case\n",
    "\n",
    "    x = df[classification_features[category]].fillna(0)\n",
    "    y = df['result']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    test_dates = df.loc[x_test.index, 'game_date']\n",
    "    test_start_dates[category] = test_dates.min()\n",
    "\n",
    "# --- STEP 3: Check for Overlap\n",
    "print(\"\\nChecking for potential overlap between regression training and classification test sets:\\n\")\n",
    "for cat in categories:\n",
    "    test_date = test_start_dates[cat]\n",
    "    if regression_cutoff_date >= test_date:\n",
    "        print(f\"{cat.upper()}: OVERLAP detected — Regression trained up to {regression_cutoff_date.date()}, classification test starts {test_date.date()}\")\n",
    "    else:\n",
    "        print(f\"{cat.upper()}: Clean split — Regression ends {regression_cutoff_date.date()}, classification test starts {test_date.date()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
